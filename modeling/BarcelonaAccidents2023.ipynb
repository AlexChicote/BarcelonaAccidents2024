{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "896d6eff-96b4-4178-8ddc-3d5665c7d75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import python_files.functions_barcelona as fb\n",
    "import datetime\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import difflib\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import date, datetime, timedelta\n",
    "#from python_files.functions_barcelona import getting_daily_weather,getting_next_day\n",
    "#rom python_files.functions_barcelona import concatenating_dataframes\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "import webbrowser\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e396693a-9657-47ad-aa7c-550784e2009e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causes DONE\n",
      "persones DONE\n",
      "tipus DONE\n",
      "vehicles DONE\n",
      " DONE\n",
      "causa_conductor DONE\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "###ORIGINAL WORKING\n",
    "\n",
    "response = requests.get('https://opendata-ajuntament.barcelona.cat/data/api/3/action/package_search?rows=1000').json()\n",
    "\n",
    "noms=['causes-', 'persones-','tipus-', 'vehicles-','','causa_conductor_']\n",
    "##MAYBE ADDING CAUSA_CONDUCTOR\n",
    "dict_files={}\n",
    "for nom in noms:\n",
    "   \n",
    "    dict_files[nom[:-1]]=fb.concatenating_dataframes(nom,response)\n",
    "    print(nom[:-1] +' DONE')\n",
    "\n",
    "with open(\"./data/dataframes_dict_files.pkl\", \"wb\" ) as f:\n",
    "    pickle.dump( dict_files, f)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ba141b2-bfe8-475d-b659-aeb550fad795",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126338, 18)\n",
      "DONE WITH ACCIDENTS\n",
      "(126335, 18) (126317, 2)\n",
      "TOTAL (126314, 19)\n",
      "DONE WITH CAUSES\n",
      "DONE WITH PEOPLE\n",
      "type:  (126320, 2) Total:  (126314, 37)\n",
      "DONE WITH TYPE\n",
      "vehicles:  (126241, 6) Total:  (126314, 42)\n",
      "DONE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./data/dataframes_dict_files.pkl\", \"rb\" ) as file:\n",
    "    dict_files=pickle.load(file)\n",
    "columnes=['num_incident', 'district_code','district', 'neighborhood','street_name',\n",
    "          'weekday', 'year', 'month', 'day', 'hour', 'ped_cause',\n",
    "           'num_deaths', 'num_minorly_injured', 'num_severly_injured',\n",
    "            'num_victims', 'num_vehicles', 'utm_y', 'utm_x']\n",
    "replacements={\"Numero_expedient\": \"num_incident\",\n",
    "              \"Numero_d_expedient\": \"num_incident\",\n",
    "              \"Numero_dexpedient\": \"num_incident\",\n",
    "              'N£mero_dexpedient': \"num_incident\",\n",
    "              'Numero_Expedient': \"num_incident\",\n",
    "              'Codi_expedient': 'num_incident',\n",
    "              'Codi_dexpedient':'num_incident',\n",
    "                \"Nom_districte\": \"district\",\n",
    "              'Codi_districte': 'district_code',\n",
    "              'Codi districte': 'district_code',\n",
    "               \"Nom_barri\":\"neighborhood\",\n",
    "              \"Nom_carrer\":\"street_name\",\n",
    "              \"Descripcio_dia_setmana\":\"weekday\",\n",
    "              \"NK_Any\":\"year\",\n",
    "              \"Any\":\"year\",\n",
    "              \"Nom_mes\":\"month\",\n",
    "              \"Dia_mes\":\"day\",\n",
    "              \"Dia_de_mes\":\"day\",\n",
    "              \"Hora_dia\":\"hour\",\n",
    "              \"Descripcio_causa_vianant\":\"ped_cause\",\n",
    "              \"Numero_morts\":\"num_deaths\",\n",
    "              \"Numero_lesionats_lleus\":\"num_minorly_injured\",\n",
    "              \"Numero_lesionats_greus\":\"num_severly_injured\",\n",
    "              \"Numero_victimes\":\"num_victims\",\n",
    "              \"Numero_vehicles_implicats\":\"num_vehicles\",\n",
    "              \"Coordenada_UTM_X_ED50\":\"utm_x\",\n",
    "              \"Coordenada_UTM_Y_ED50\":\"utm_y\",\n",
    "            \"Coordenada_UTM_X\":\"utm_x\",\n",
    "             \"Coordenada_UTM_Y\":\"utm_y\",\n",
    "             \"Coordenada UTM (Y)\": \"utm_y\",\n",
    "             \"Coordenada UTM (X)\": \"utm_x\",\n",
    "             \"Coordenada_UTM_(Y)\":\"utm_y\",\n",
    "             \"Coordenada_UTM_(X)\":\"utm_x\",\n",
    "             \"Longitud_WGS84\":\"Longitud\",\n",
    "             \"Latitud_WGS84\":\"Latitud\",\n",
    "             'Num_postal_':\"Num_postal\",\n",
    "             \"Num_postal_caption\":\"Num_postal\",\n",
    "             'Tipus_accident':'accident_type',\n",
    "              'Descripcio_tipus_accident':'accident_type',\n",
    "              'Descripcio_color':'vehicle_color',\n",
    "              'Descripcio_model':'vehicle_model',\n",
    "              'Descripcio_marca':'vehicle_brand_name',\n",
    "              'Descripcio_carnet': 'license',\n",
    "              'Antiguitat_carnet':'license_seniority'\n",
    "              \n",
    "             }\n",
    "\n",
    "llista_=['causes', 'persones', 'tipus', 'vehicles', '']\n",
    "for grup in llista_:\n",
    "    for num, df in enumerate(dict_files[grup]):\n",
    "        df.columns=[col.replace(\"ú\",'u').replace(\"ó\",\"o\").replace(\"'\",'').replace(' ','_').replace(\"_de_\",\"_\").replace(\"í\",\"i\").replace(\"¢\",'o') for col in df]\n",
    "\n",
    "        df=df.rename(columns=replacements,)\n",
    "        dict_files[grup][num]=df\n",
    "acc=pd.DataFrame(columns=columnes)\n",
    "    \n",
    "for dd in dict_files['']:\n",
    "    columns=[col for col in dd.columns if col in columnes]\n",
    "    dd_final=dd[columns]\n",
    "    acc=pd.concat([acc,dd_final])\n",
    "    #print(acc.shape)\n",
    "\n",
    "\n",
    "##Fixing utm_x and utm_y that are mixed in some cases. Replacing nulls(-1) with the mean\n",
    "acc['utm_x']=[x[0] if len(str(x[0]).split('.')[0])==7 else x[1] for x in zip(acc.utm_x,acc.utm_y)]\n",
    "acc['utm_y']=[x[1] if len(str(x[1]).split('.')[0])==6 else x[0] for x in zip(acc.utm_x,acc.utm_y)]\n",
    "\n",
    "##Fixing other stuff\n",
    "acc['ped_cause']=acc['ped_cause'].apply(fb.ped_to_angles)\n",
    "acc['weekday']=acc['weekday'].apply(fb.setmana_a_angles)\n",
    "acc['month']=acc['month'].apply(fb.mes_a_angles)\n",
    "acc['num_incident']=[x.strip() for x in acc['num_incident']]\n",
    "#print(acc.shape)\n",
    "acc=acc.drop_duplicates(subset='num_incident',keep='last')\n",
    "print(acc.shape)\n",
    "\n",
    "\n",
    "dups=acc[acc['num_incident'].duplicated(keep=False)]\n",
    "dups[dups[['num_incident','ped_cause']].duplicated()]\n",
    "##all duplicates are due to have 2 different ped cause being the rest exactly the same. Total dups: 27. I will just keep the first. No big deal; does not pay wasting time\n",
    "acc=acc.dropna()\n",
    "acc['district_code']=[str(int(x)) if x <0 or x>9 else '0'+str(int(x)) for x in acc.district_code]\n",
    "acc.to_csv('./data/accidents_only2023.csv',index=False)\n",
    "#print(acc.isnull().sum())\n",
    "print(\"DONE WITH ACCIDENTS\")\n",
    "\n",
    "\n",
    "#CAUSES\n",
    "\n",
    "for num, df in enumerate(dict_files['causes']):\n",
    "    columns_to_save=[]\n",
    "    for col in df:\n",
    "        if ('incident' in col.lower()) or ('mediata' in col.lower()):\n",
    "            columns_to_save.append(col)\n",
    "    \n",
    "    dict_files['causes'][num]=df[columns_to_save]\n",
    "    dict_files['causes'][num].columns=['num_incident' if 'incident' in col else 'cause' for col in columns_to_save]\n",
    "    \n",
    "causes=pd.concat(dict_files['causes']).reset_index(drop=True)\n",
    "causes['num_incident']=[x.strip() for x in causes['num_incident']]\n",
    "causes['cause']=causes.cause.apply(fb.posant_accents).apply(fb.cause_to_angles)\n",
    "causes=causes.drop_duplicates('num_incident',keep='last')\n",
    "causes.to_csv('./data/causes2023.csv')\n",
    "print(acc.shape,causes.shape)\n",
    "total= pd.merge(acc,causes, how='inner',on='num_incident')\n",
    "print('TOTAL',total.shape)\n",
    "print(\"DONE WITH CAUSES\")\n",
    "\n",
    "#descripcio situacio: no interessa, no te valor predictiu\n",
    "# No intereest either on 'Descripcio_victimitzacio','Descripcio_Motiu_desplaçament_vianant','Descripcio_Motiu_desplaçament_conductor'\n",
    "chosen_features=['num_incident','Descripcio_sexe','Edat','Descripcio_tipus_persona','Descripcio_tipus_persona','Desc_Tipus_vehicle_implicat']\n",
    "\n",
    "mapping_columns={'num_incident':'num_incident',\n",
    "             'Descripcio_sexe':'gender',\n",
    "             'Edat':'age',\n",
    "            'Descripcio_tipus_persona':'people_role',\n",
    "                'Desc_Tipus_vehicle_implicat': 'vehicle',\n",
    "                'Desc._Tipus_vehicle_implicat': 'vehicle',}\n",
    "persones=[]\n",
    "\n",
    "for d in dict_files['persones']:\n",
    "    #print(d.columns)\n",
    "    d.columns=[fb.posant_accents(col) for col in d.columns]\n",
    "    d.columns=[col if not col.startswith('Desc._') else 'Desc_Tipus_vehicle_implicat' for col in d.columns]\n",
    "    n_columns=[col for col in d.columns if col in chosen_features]\n",
    "    f=d[n_columns].copy()\n",
    "    f.columns=[mapping_columns[columna] for columna in f.columns]\n",
    "    persones.append(f)\n",
    "    \n",
    "persones=pd.concat(persones)\n",
    "persones=persones.reset_index()\n",
    "persones['num_incident']=[x.strip() for x in persones.num_incident]\n",
    "persones=persones.replace('Dona',1).replace('Home',0).replace(\"Conductor\",'driver').replace('Vianant', 'pedestrian').replace(\"Passatger\",'passenger').replace('Desconegut',np.NaN).replace(-1,np.NaN)\n",
    "persones['age']=persones['age'].astype(float)\n",
    "persones['vehicle']=persones.vehicle.map(fb.map_vehicles)\n",
    "for ix in persones[(persones.people_role=='driver')&(persones.age==0)].index:\n",
    "    persones.loc[ix,'age']=np.NaN\n",
    "for ix in persones[(persones.age>90)&(persones.people_role=='driver')].index:\n",
    "    persones.loc[ix,'age']=np.NaN\n",
    "persones['vehicle']=persones.vehicle.fillna('unknown')\n",
    "persones['vehicle']=['truck' if 'truck' in x else 'bus' if ' bus' in x or x=='tram' else 'car' if x=='suv' else 'van' if 'minibus' in x else x for x in persones.vehicle]\n",
    "persones['vehicle']=persones.vehicle.replace('unkown',np.NaN)\n",
    "\n",
    "##TODO: once i fix the age issue based in the vehicle\n",
    "misc_vehicles=persones.vehicle.value_counts()[persones.vehicle.value_counts()<1000].index\n",
    "persones['vehicle']=[veh if veh not in misc_vehicles else 'misc_vehicle' for veh in persones.vehicle]\n",
    "\n",
    "people=persones.set_index('num_incident').drop('index',axis=1)\n",
    "\n",
    "people=pd.get_dummies(people)\n",
    "\n",
    "index_1=people.index\n",
    "vehicle_cols=[col for col in people.columns if 'vehicle_' in col]\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "people = pd.DataFrame(scaler.fit_transform(people), columns = people.columns)\n",
    "\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "people = pd.DataFrame(imputer.fit_transform(people),columns = people.columns)\n",
    "\n",
    "people=pd.DataFrame(scaler.inverse_transform(people), columns = people.columns)\n",
    "vehicle_df=people[vehicle_cols].copy()\n",
    "#people=people.drop(vehicle_cols,axis=1)\n",
    "#people.isnull().sum()\n",
    "people['gender_100/1']=[100 if gen<=0.5 else 1 for gen in people.gender]\n",
    "people['age_driver']=people.age*people.people_role_driver\n",
    "people['gender_driver']=people['gender_100/1']*people.people_role_driver\n",
    "if 'gender' in list(people.columns):\n",
    "    people=people.drop('gender',axis=1)\n",
    "people=people.set_index(index_1)\n",
    "people['driver_u_25']=[1 if x < 27 else 0 for x in people.age_driver]\n",
    "people=people.reset_index().groupby('num_incident').sum()\n",
    "people['age_driver']=[x[0]/x[1] if (x[1]!=0) else np.nan for x in zip(people['age_driver'],people['people_role_driver'])]\n",
    "people['gender_driver_male']=[int(str(gen)[0]) if len(str(gen))==3 else 0 for gen in people.gender_driver.astype(int)]\n",
    "people['gender_driver_female']=[int(str(gen)[2]) if len(str(gen))==3 else gen for gen in people.gender_driver.astype(int)]\n",
    "people=people.drop(['age','gender_100/1','gender_driver'],axis=1)\n",
    "#people.merge()\n",
    "\n",
    "total= pd.merge(total,people, how='left',on='num_incident')\n",
    "total['age_driver']=total.age_driver.fillna(total.age_driver.mean())\n",
    "#print(total.isnull().sum())\n",
    "people.to_csv('./data/people2023.csv', index=False)\n",
    "\n",
    "### NEXT STEP: What to do with the nulls in total after merging with people?WILL WAIT UNTIL FINAL TOTAL\n",
    "\n",
    "print(\"DONE WITH PEOPLE\")\n",
    "\n",
    "###TYPE\n",
    "\n",
    "\n",
    "tipus=pd.DataFrame(columns=['num_incident','accident_type'])\n",
    "\n",
    "for df in dict_files['tipus'].copy():\n",
    "    \n",
    "    tipus=pd.concat([tipus,df[['num_incident','accident_type']]])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "type_accident_map={'Atropellament': 'run_over',\n",
    "         'Col.lisio lateral': 'lateral_collision',\n",
    "        'Xoc contra element estatic': 'crash_into_stationary',\n",
    "      'Abast': 'rear-end_collision',\n",
    "       'Col.lisio frontal':'frontal_collision',\n",
    "      'Col.lisio fronto-lateral':'frontal-lateral_collision',\n",
    "      'Caiguda (dues rodes)':'fall--motorcycle',\n",
    "      'Abast multiple':'multiple_rear-end_collision',\n",
    "      'Caiguda interior vehicle':'fall_inside_vehicle',\n",
    "      'Altres':'Other_types',\n",
    "      'Bolcada (mes de dues rodes)':'overturning',\n",
    "      'Desconegut':'unknown',\n",
    "      'Sortida de via amb xoc o col.lisio':'run-off_with_crash_or_collision',\n",
    "      'Encalç':'rear-end_collision',\n",
    "      'Sortida de via amb bolcada':'run-off_with_overturning',\n",
    "      'Xoc amb animal a la calçada':'crash_into_animal_on_road',\n",
    "      'Resta sortides de via':'run-off_not_included_previously'}\n",
    "tipus.shape\n",
    "\n",
    "tipus['accident_type']=tipus['accident_type'].apply(fb.posant_accents).map(type_accident_map)\n",
    "\n",
    "tipus['num_incident']=[x.strip() for x in tipus.num_incident]\n",
    "tipus=tipus.dropna()\n",
    "misc_type=list(tipus.accident_type.value_counts()[tipus.accident_type.value_counts()<350].index)\n",
    "misc_type.append('Other_types')\n",
    "tipus=tipus.groupby('num_incident')['accident_type'].agg(lambda x: ','.join(x)).reset_index()\n",
    "tipus['accident_type']=[\"misc_type\" if x in misc_type else 'frontal' if 'frontal' in x else 'collision' if 'collision' in x else 'crash' if 'crash' in x else x for x in tipus.accident_type]\n",
    "\n",
    "\n",
    "type_list=tipus.accident_type.value_counts().index\n",
    "tipus['accident_type']=[fb.organizing_types(x,type_list) for x in  tipus.accident_type]\n",
    "tipus2=tipus.copy()\n",
    "total4=total.copy()\n",
    "total= pd.merge(total,tipus, how='left',on='num_incident')\n",
    "#total.fillna('misc_type',inplace=True)\n",
    "total5=total.copy()\n",
    "tipus.to_csv('./data/types2023.csv',index=False)\n",
    "\n",
    "print('type: ', tipus.shape, 'Total: ',total.shape)\n",
    "\n",
    "print(\"DONE WITH TYPE\")\n",
    "\n",
    "##VEHICLE***Model no surt a tots els anys\n",
    "\n",
    "columns_to_add=['num_incident', 'vehicle_model', 'vehicle_brand_name',\n",
    "       'vehicle_color', 'license', 'license_seniority' ]\n",
    "vehicles=pd.DataFrame(columns=columns_to_add)\n",
    "\n",
    "for df in dict_files['vehicles'].copy():\n",
    "    \n",
    "    vehicles=pd.concat([vehicles,df[columns_to_add]])\n",
    "    \n",
    "vehicles=vehicles.dropna()\n",
    "vehicles['num_incident']=[x.strip() for x in vehicles.num_incident]\n",
    "vehicles2=vehicles.copy()\n",
    "vehicles=vehicles.groupby('num_incident').agg(lambda x: ','.join(x)).reset_index()\n",
    "vehicles.to_csv('./data/vehicles2023.csv',index=False)\n",
    "total= pd.merge(total,vehicles, how='left',on='num_incident')\n",
    "##I will input the most common one\n",
    "total=total.apply(lambda x: x.fillna(x.value_counts().index[0]))\n",
    "\n",
    "total.to_csv('./data/accidents2023.csv',index=False)\n",
    "#total.fillna(-1,inplace=True)\n",
    "print('vehicles: ', vehicles.shape, 'Total: ',total.shape)\n",
    "print('DONE')\n",
    "\n",
    "\n",
    "\n",
    "### NEXT STEP: What to do with the nulls in total after merging with people?WILL WAIT UNTIL FINAL TOTAL\n",
    "url = \"https://www.youtube.com/watch?v=Udt-9J8nzGE\"\n",
    "webbrowser.open(url,new=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
