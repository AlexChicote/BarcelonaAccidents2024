{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "\n",
    "# from scipy.stats import ttest_ind, chi2_contingency\n",
    "#!pip install skfeature-chappers\n",
    "# from skfeature.function.similarity_based import fisher_score\n",
    "\n",
    "# from sklearn.metrics import recall_score, accuracy_score,confusion_matrix,roc_auc_score,precision_score\n",
    "# from sklearn.dummy import DummyClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "#,cross_val_score,KFold, StratifiedKFold,\\\n",
    "# GridSearchCV,cross_validate, RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest,f_classif,chi2, mutual_info_classif, VarianceThreshold\n",
    "# from sklearn.pipeline import Pipeline,make_pipeline\n",
    "# from sklearn.preprocessing import StandardScaler,Normalizer, MinMaxScaler, PowerTransformer, OneHotEncoder,OrdinalEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.linear_model import Perceptron\n",
    "# from sklearn.svm import SVC, LinearSVC\n",
    "# #import functions_barcelona_2 as fb2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from statsmodels.graphics.mosaicplot import mosaic\n",
    "# from itertools import product\n",
    "# import category_encoders as ce\n",
    "\n",
    "\n",
    "# from yellowbrick.classifier import ROCAUC\n",
    "# from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
    "# from imblearn.under_sampling import RandomUnderSampler,NearMiss\n",
    "from imblearn.pipeline import Pipeline as Pipeline_imb\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "\n",
    "import python_files.functions_barcelona_Correlations as fb3\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 12, 32, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accidents=pd.read_csv('./data/accidents_weather.csv')\n",
    "def creating_target(row):\n",
    "    return 1 if row['num_deaths']+row['num_severly_injured']>0 else 0\n",
    "accidents['target']=accidents.apply(creating_target,axis=1)\n",
    "\n",
    "\n",
    "cat_columns=[]\n",
    "num_columns=[]\n",
    "to_drop=['index','num_incident', 'num_severly_injured','num_deaths',\\\n",
    "         'num_minorly_injured','num_victims','street_code', 'dates','datetimes','utm_x','utm_y']\n",
    "for col in accidents:\n",
    "    if 'Unnamed' in col:\n",
    "        accidents=accidents.drop(col,axis=1)\n",
    "for col in accidents.columns:\n",
    "    if col in to_drop:\n",
    "        accidents=accidents.drop(col,axis=1)\n",
    "    elif ('vehicle_' in col) or ('license' in col):\n",
    "        accidents=accidents.drop(col,axis=1)\n",
    "    elif col =='target':\n",
    "        pass\n",
    "    elif 'num' in col:\n",
    "        num_columns.append(col)\n",
    "    elif accidents[col].nunique()<76:\n",
    "        cat_columns.append(col)\n",
    "    elif accidents[col].dtypes==float:\n",
    "        num_columns.append(col)\n",
    "len(cat_columns),len(num_columns),len(accidents.columns),len(accidents.columns)-len(cat_columns)-len(num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "street_name object 10716\n",
      "datetime object 67652\n",
      "target int64 2\n"
     ]
    }
   ],
   "source": [
    "pending=[x for x in accidents.columns if x not in cat_columns+num_columns]\n",
    "for col in pending:\n",
    "    print(col,accidents[col].dtypes, accidents[col].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELECTING NUMERICAL ONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_avg 1.0 temp_min\n",
      "temp_max 1.0 temp_avg\n",
      "temp_min 1.0 temp_avg\n",
      "windspeed 0.87 max_windspeed\n",
      "max_windspeed 0.87 windspeed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['num_vehicles',\n",
       " 'age_driver',\n",
       " 'temp_avg',\n",
       " 'relative_humidity',\n",
       " 'precipitation',\n",
       " 'windspeed',\n",
       " 'wind_direction',\n",
       " 'pressure',\n",
       " 'solar_radiation']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#correlations amongst numerical \n",
    "\n",
    "correlations= abs(accidents[num_columns].corr())\n",
    "correlations=correlations.replace(1,0)\n",
    "for col in num_columns:\n",
    "    #print(correlations[col])\n",
    "    if correlations[col].max()>0.8:\n",
    "        print(col, round(correlations[col].max(),2),correlations[col].sort_values(ascending=False).index[0])\n",
    "#They are strongly correlated therefore I eliminate temp_min,temp_max and max_windspeed\n",
    "to_remove=['temp_min','temp_max','max_windspeed']\n",
    "for col in to_remove:\n",
    "    if col in num_columns:\n",
    "        num_columns.remove(col)\n",
    "num_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['num_vehicles',\n",
       " 'age_driver',\n",
       " 'temp_avg',\n",
       " 'relative_humidity',\n",
       " 'precipitation',\n",
       " 'windspeed',\n",
       " 'wind_direction',\n",
       " 'pressure',\n",
       " 'solar_radiation']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature sel ===============>>VarianceThreshold\n",
    "vt=VarianceThreshold()\n",
    "vt.fit(X_train)\n",
    "num_columns=[num_columns[col[0]] for col in enumerate(vt.get_support()) if col[1]]\n",
    "num_columns ##=======>>No changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###SELECTING NUMERICAL ONES\n",
    "X=accidents[num_columns].copy()\n",
    "y=accidents.target\n",
    "\n",
    "# feature selection=========>>Using ANOVA\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    # configure to select all features\n",
    "    fs = SelectKBest(score_func=f_classif, k='all')\n",
    "    # learn relationship from training data\n",
    "    fs.fit(X_train, y_train)\n",
    "    # transform train input data\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    # transform test input data\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    " \n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n",
    "# what are scores for the features\n",
    "sorted_columns=sorted([x for x in zip(num_columns,fs.scores_)], key=lambda x: x[1],reverse=True)\n",
    "# for i in range(len(sorted_columns)):\n",
    "#     print(f'Feature {sorted_columns[i][0]}:{sorted_columns[i][1]}')\n",
    "# plot the scores\n",
    "cols=[a for a,b in sorted_columns]\n",
    "scores=[b for a,b in sorted_columns]\n",
    "plt.barh(cols[::-1], scores[::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['num_vehicles', 'age_driver', 'solar_radiation']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anova_sel=[col[0] for col in sorted_columns[:3]]\n",
    "anova_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['num_vehicles', 'age_driver', 'solar_radiation']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature selection=========>>Using Pearson\n",
    "pearson_sel=list(abs(accidents[num_columns +['target']].corr()['target']).sort_values(ascending=False)[1:4].index)\n",
    "pearson_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['num_vehicles', 'solar_radiation', 'age_driver']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature selection=========>>Using Kendall\n",
    "kendall_sel=list(abs(accidents[num_columns +['target']].corr(method='kendall')['target']).sort_values(ascending=False)[1:4].index)\n",
    "kendall_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['num_vehicles', 'solar_radiation', 'age_driver']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature selection=========>>Using Spearman\n",
    "spearman_sel=list(abs(accidents[num_columns +['target']].corr(method='spearman')['target']).sort_values(ascending=False)[1:4].index)\n",
    "spearman_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['num_vehicles', 'age_driver', 'solar_radiation']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature sel ===============>>Information Gain\n",
    "importances=mutual_info_classif(X_train, y_train)\n",
    "info_gain_sel=list(pd.DataFrame(importances,columns=['InformationGain'],index=num_columns).sort_values('InformationGain',ascending=False).index[:3])\n",
    "info_gain_sel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Numerical selected\n",
    "numerical_chosen=set(anova_sel+pearson_sel+spearman_sel+kendall_sel+info_gain_sel)\n",
    "model_dict={}\n",
    "model_dict['numerical_features']=numerical_chosen\n",
    "with open('./data/model_charac.pkl', 'wb') as f:\n",
    "    pickle.dump(model_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_up=accidents.target.value_counts(normalize=True).values[0]\n",
    "baseline_down=accidents.target.value_counts(normalize=True).values[1]\n",
    "baseline_up,baseline_down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing num_vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents['num_vehicles_aggr']=[x if x <7 else 'the rest' for x in accidents.num_vehicles]\n",
    "df_num_vehicles=pd.crosstab(accidents.target,accidents.num_vehicles_aggr, normalize='columns')\n",
    "df_num_vehicles.style.map(lambda x: 'background-color : red' if x<baseline_up and x>baseline_down else 'background-color :green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents['num_vehicles_aggr']=[x if x <7 else 'the rest' for x in accidents.num_vehicles]\n",
    "df_num_vehicles=pd.crosstab(accidents.target,accidents.num_vehicles_aggr, normalize='columns')\n",
    "df_num_vehicles.style.map(lambda x: 'background-color : green' if x>baseline_up or x<baseline_down else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing only 1 and 2 vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_accidents=accidents[accidents.num_vehicles.isin([1,2])]\n",
    "colors={}\n",
    "for j in range(new_accidents.num_vehicles_aggr.nunique()):\n",
    "    rand_colors = [\"#\"+''.join([random.choice('ABCDEF0123456789') for i in range(6)])]\n",
    "    colors[j]=rand_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = {}\n",
    "for x in [0, 1]:\n",
    "    for y, col in colors.items():\n",
    "        props[(x, y)] ={'color': col}\n",
    "        \n",
    "mosaic(new_accidents, ['target', 'num_vehicles_aggr'],\n",
    "       labelizer=lambda k: '',\n",
    "       properties=props);\n",
    "# props={}\n",
    "# props[(0,'Yes')]={'facecolor':'red', 'edgecolor':'white'}\n",
    "# props[(0,'No')]={'facecolor':'red', 'edgecolor':'white'}\n",
    "# props[(1,'Yes')]={'facecolor':'xkcd:aqua','edgecolor':'white'}\n",
    "# props[(1,'No')]=        {'facecolor':'xkcd:aqua','edgecolor':'white'}\n",
    "# labelizer=lambda k:{(0,'Yes'):357,(1,'Yes'):31,(0,'No'):130,(1,'No'):80}[k]\n",
    "# mosaic(accidents,['target','num_vehicles_aggr'],labelizer=labelizer,properties=props)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(accidents.target,accidents.num_vehicles_aggr,normalize='columns').iloc[1].plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents[accidents.age_driver==-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(x='target',y='age_driver',data=accidents);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age=accidents.age_driver\n",
    "age_1=accidents[accidents['target'] == 1].age_driver\n",
    "age_0=accidents[accidents['target'] == 0].age_driver\n",
    "age_0.mean(), age_1.mean(),age.mean(),age_0.std(), age_1.std(),age.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(accidents[accidents.age_driver>0][accidents.target==0].age_driver,);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents[(accidents['target'] == 0) & (accidents.age_driver>0)].age_driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(accidents[(accidents['target'] == 0) & (accidents.age_driver>0)].age_driver);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(accidents[(accidents['target'] == 1) & (accidents.age_driver>0)].age_driver);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(accidents[accidents['target'] == 0].age_driver, label='Not', color='green', alpha=0.3, bins='scott')\n",
    "plt.hist(accidents[accidents['target'] == 1].age_driver, label='Severe', color='red', alpha=0.5, bins=14)\n",
    "plt.legend()\n",
    "plt.title('Histogram of Age target')\n",
    "plt.xlabel(\"Driver's Age\");\n",
    "sns.histplot(data=accidents, x=\"age_driver\", hue=\"target\", multiple='layer', element='step', binwidth=2, palette=['red', 'green']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Two-Sided T-Test comparing mean values of Loan and Non-loan classes\n",
    "print('Probability value of age_driver coming from same distribution:',\n",
    "      round(ttest_ind(accidents[(accidents['target'] == 0) & (accidents.age_driver>0)].age_driver,\\\n",
    "                                accidents[(accidents['target'] == 1) & (accidents.age_driver>0)].age_driver).pvalue, 4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function for quickly plotting categorical features\n",
    "def cat_pairplot(plt_data=accidents[num_columns], y_feat='target'):\n",
    "    # Create a list of all feature names\n",
    "    feat_cols = list(plt_data.columns)\n",
    "    \n",
    "    # Make each plot on a pairplot-like graph with subplots, 2 wide\n",
    "    num_rows = int(len(feat_cols)/2)\n",
    "    f, axs = plt.subplots(nrows=num_rows, ncols=2,\n",
    "                          figsize=(14, 7*num_rows),\n",
    "                          sharey='row')\n",
    "    # Add additional vertical space between subplots\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    \n",
    "    # Iterate and illustrate a histogram for each categorical feature, colored with response\n",
    "    for i in range(len(feat_cols)):\n",
    "        sns.histplot(ax=axs[int(i/2), i%2], data=plt_data, x=feat_cols[i], hue=y_feat, multiple=\"dodge\", shrink=0.8)\n",
    "        axs[int(i/2), i%2].set_title(f'Histogram of \\'{feat_cols[i]}\\' Status against Voter Response');\n",
    "        # Rotat x-ticks for better readability\n",
    "        axs[int(i/2), i%2].tick_params(labelrotation=20, pad=1)\n",
    "        \n",
    "cat_pairplot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scores,index=cols,columns=['f_class_target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['district_code',\n",
       " 'district',\n",
       " 'neighborhood',\n",
       " 'weekday',\n",
       " 'year',\n",
       " 'month',\n",
       " 'day',\n",
       " 'hour',\n",
       " 'ped_cause',\n",
       " 'cause',\n",
       " 'people_role_driver',\n",
       " 'people_role_passenger',\n",
       " 'people_role_pedestrian',\n",
       " 'driver_u_25',\n",
       " 'gender_driver_male',\n",
       " 'gender_driver_female',\n",
       " 'accident_type']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('district_code', 'district') cramers 1.0\n",
      "('district_code', 'district') theils 1.0\n",
      "('district_code', 'district') theils 1.0\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Correlations amongst cat features\n",
    "from itertools import permutations\n",
    "cat_columns\n",
    "#cat_columns.remove('weather_summary')--Temporarily until a bucket them\n",
    "###Permutations: order matters\n",
    "###Combinartions: Order does not matter\n",
    "removed_columns=[]\n",
    "\n",
    "for perm in [x for x in permutations(cat_columns,2)]:\n",
    "    if (perm[0] in removed_columns) or (perm[1] in removed_columns):\n",
    "        break\n",
    "    correlation_c=fb3.cramers_corrected_stat(accidents[perm[0]],accidents[perm[1]])\n",
    "    correlation_u=fb3.theils_u(accidents[perm[0]],accidents[perm[1]])\n",
    "    if correlation_c >0.8:\n",
    "        print(perm, 'cramers', correlation_c)\n",
    "        if correlation_u >0.8:\n",
    "            print(perm,'theils', correlation_u)\n",
    "            removed_columns.append(perm[0])\n",
    "        \n",
    "    if correlation_u >0.8:\n",
    "        print(perm,'theils', correlation_u)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns=[col for col in cat_columns if col not in removed_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contingency tables ans Stacked column charts!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###SELECTING CATEGORICAL ONES\n",
    "accidents['people_role_pass']=[x if x<6 else 10 for x in accidents.people_role_pass]\n",
    "accidents['vehicles_bus']=[x if x<7 else 10 for x in accidents.vehicles_bus]\n",
    "accidents['vehicles_car']=[x if x<7 else 10 for x in accidents.vehicles_car]\n",
    "accidents['vehicles_misc_vehicle']=[x if x<6 else 10 for x in accidents.vehicles_car]\n",
    "accidents['vehicles_motorcycle']=[x if x<4 else 7 for x in accidents.vehicles_motorcycle]\n",
    "accidents['vehicles_van']=[x if x<3 else 6 for x in accidents.vehicles_van]\n",
    "accidents['gender_driver_female']=[x if x<3 else 3 for x in accidents.gender_driver_female]\n",
    "\n",
    "X=accidents[cat_columns].copy()\n",
    "#X['people_role_pass']=[x if x<6 else 10 for x in X.people_role_pass]\n",
    "# X['vehicles_bus']=[x if x<7 else 10 for x in X.vehicles_bus]\n",
    "# X['vehicles_car']=[x if x<7 else 10 for x in X.vehicles_car]\n",
    "# X['vehicles_misc_vehicle']=[x if x<6 else 10 for x in X.vehicles_car]\n",
    "# X['vehicles_motorcycle']=[x if x<4 else 7 for x in X.vehicles_motorcycle]\n",
    "# X['vehicles_van']=[x if x<3 else 6 for x in X.vehicles_van]\n",
    "# X['gender_driver_female']=[x if x<3 else 3 for x in X.gender_driver_female]\n",
    "# y=accidents.target\n",
    "\n",
    "def prepare_inputs(X_train, X_test):\n",
    "    oe = OrdinalEncoder()\n",
    "    oe.fit(X_train)\n",
    "    X_train_enc = oe.transform(X_train)\n",
    "    X_test_enc = oe.transform(X_test)\n",
    "    return X_train_enc, X_test_enc\n",
    "def select_features(X_train, y_train, X_test):\n",
    "    fs = SelectKBest(score_func=chi2, k='all')\n",
    "    fs.fit(X_train, y_train)\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    X_test_fs = fs.transform(X_test)\n",
    "    return X_train_fs, X_test_fs, fs\n",
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1,stratify=y)\n",
    "# prepare inputs\n",
    "X_train,X_test=prepare_inputs(X_train,X_test)\n",
    "# feature selection\n",
    "X_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n",
    "sorted_columns=sorted([x for x in zip(cat_columns,fs.scores_)], key=lambda x: x[1],reverse=True)\n",
    "# for i in range(len(sorted_columns)):\n",
    "#     print(f'Feature {sorted_columns[i][0]}:{sorted_columns[i][1]}')\n",
    "# plot the scores\n",
    "cols=[a for a,b in sorted_columns]\n",
    "scores=[b for a,b in sorted_columns]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.barh(cols[::-1], scores[::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/model_charac.pkl', 'rb') as f:\n",
    "    data_pickled=pickle.load(f)\n",
    "data_pickled.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Numerical selected\n",
    "\n",
    "model_dict['categorical_features']=cols[:5]\n",
    "model_dict['categorical_features']\n",
    "with open('../data/model_charac.pkl', 'wb') as f:\n",
    "    pickle.dump(model_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "model_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents['people_role_pass'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cramers, theils = [],[]\n",
    "for col in cat_columns:\n",
    "    correlation_c=fb3.cramers_corrected_stat(accidents[col],accidents.target)\n",
    "    correlation_u=fb3.theils_u(accidents[col],accidents.target)\n",
    "    #print(col, 'cramers', correlation_c)\n",
    "    cramers.append((col,correlation_c))\n",
    "    #print(col,'theils', correlation_u)\n",
    "    theils.append((col,correlation_u))\n",
    "sorted(cramers,key=lambda x:x[1],reverse=True)[0:6], sorted(theils,key=lambda x:x[1],reverse=True)[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(theils,key=lambda x:x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corre_cat_col=cols[:5]\n",
    "###Permutations: order matters\n",
    "###Combinartions: Order does not matter\n",
    "from itertools import permutations\n",
    "for perm in [x for x in permutations(corre_cat_col,2)]:\n",
    "    correlation_c=cramers_corrected_stat(accidents[perm[0]],accidents[perm[1]])\n",
    "    correlation_u=theils_u(accidents[perm[0]],accidents[perm[1]])\n",
    "    if correlation_c >0.8:\n",
    "        print(perm, 'cramers', correlation_c)\n",
    "    if correlation_u >0.8:\n",
    "        print(perm,'theils', correlation_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=accidents[cat_columns].copy()\n",
    "y=accidents.target\n",
    "def prepare_inputs(X_train, X_test):\n",
    "    oe = OrdinalEncoder()\n",
    "    oe.fit(X_train)\n",
    "    X_train_enc = oe.transform(X_train)\n",
    "    X_test_enc = oe.transform(X_test)\n",
    "    return X_train_enc, X_test_enc\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "prepare_inputs(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_dict=fb2.feature_dict\n",
    "scale_pipe = make_pipeline(StandardScaler())\n",
    "log_pipe = make_pipeline(PowerTransformer())\n",
    "categorical_pipe = make_pipeline(OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n",
    "to_scale=[fea for fea in selected_features if fea in feature_dict['numerical_features']]\n",
    "to_ohe=[fea for fea in selected_features if fea in feature_dict['ordinal_categorical_features']+feature_dict['cardinal_categorical_features']]\n",
    "\n",
    "transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"scale\", scale_pipe, to_scale),\n",
    "        #(\"log_transform\", log_pipe, to_log),\n",
    "        (\"oh_encode\", categorical_pipe, to_ohe),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb2.models_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores_df=pd.DataFrame()\n",
    "kf = RepeatedStratifiedKFold(n_splits=5, random_state=random_state, )\n",
    "\n",
    "for model in fb2.models_list:\n",
    "    pipe = Pipeline_imb([('resample', fb2.over.SMOTE(random_state=random_state)), \n",
    "                 ('clf', model[1])])\n",
    "    final_pipe=Pipeline([('transformer', transformer),('pipe',pipe)])\n",
    "    scores=fb2.cross_validate(final_pipe, X_train, y_train, scoring=fb2.metrics, cv=kf)\n",
    "    for key in scores:\n",
    "        scores_df.loc[model[0],key]=scores[key].mean() \n",
    "    print(model[0])\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##trying different models\n",
    "# X_train_std,X_test_std=fb2.preprocessing_fetures(train,test,numerical_features,categorical_features,scaler=True)\n",
    "# print(X_train_std.shape,X_test_std.shape)\n",
    "scores_df=pd.DataFrame()\n",
    "kf = RepeatedStratifiedKFold(n_splits=5, random_state=random_state, )\n",
    "for model in fb2.models_list:\n",
    "    pipe = Pipeline_imb([(\"prep\", transformer),('resample', fb2.over.SMOTE(random_state=random_state)), \n",
    "                 ('clf', model[1])])\n",
    "    scores=fb2.cross_validate(pipe, X_train.values, y_train.values, scoring=fb2.metrics, cv=kf)\n",
    "    for key in scores:\n",
    "        scores_df.loc[model[0],key]=scores[key].mean() \n",
    "    print(model[0])\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##using only randomforest with oversampling\n",
    "X_train_std,X_test_std=fb2.preprocessing_fetures(train,test,numerical_features,categorical_features,scaler=True)\n",
    "scores_over=pd.DataFrame()\n",
    "kf = StratifiedKFold(n_splits=5, random_state=random_state, shuffle=True)\n",
    "for sampling in fb2.oversamplings.keys():\n",
    "    print(sampling)\n",
    "#fb2.models_list[1][1]\n",
    "    pipe = Pipeline([('resample', fb2.oversamplings[sampling]),\n",
    "                      \n",
    "                     ('clf', fb2.models_list[1][1])])\n",
    "    scores=fb2.cross_validate(pipe, X_train_std, y_train, scoring=fb2.metrics, cv=kf)\n",
    "    for key in scores:\n",
    "        scores_over.loc[sampling,key]=scores[key].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pickled['results_uver_rf_pipeline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##using only randomforest with undersampling\n",
    "scores_under=pd.DataFrame()\n",
    "kf = StratifiedKFold(n_splits=5, random_state=random_state, shuffle=True)\n",
    "for sampling in fb2.undersamplings.keys():\n",
    "    print(sampling)\n",
    "#fb2.models_list[1][1]\n",
    "    pipe = Pipeline([('resample', fb2.undersamplings[sampling]), \n",
    "                     ('clf', fb2.models_list[1][1])])\n",
    "    scores=fb2.cross_validate(pipe, X_train_std, y_train, scoring=fb2.metrics, cv=kf)\n",
    "    for key in scores:\n",
    "        scores_under.loc[sampling,key]=scores[key].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##using only randomforest with over under\n",
    "\n",
    "scores_combo=pd.DataFrame()\n",
    "kf = StratifiedKFold(n_splits=5, random_state=random_state, shuffle=True)\n",
    "for sampling in fb2.over_undersamplings.keys():\n",
    "    print(sampling)\n",
    "#fb2.models_list[1][1]\n",
    "    pipe = Pipeline([('resample', fb2.over_undersamplings[sampling]),\n",
    "                     ('standarization', StandardScaler()), \n",
    "                     ('clf', fb2.models_list[1][1])])\n",
    "    scores=fb2.cross_validate(pipe, X_train, y_train, scoring=fb2.metrics, cv=kf)\n",
    "    for key in scores:\n",
    "        scores_combo.loc[sampling,key]=scores[key].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "validate_scores={}\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "pipe = Pipeline([('resample', fb2.over.SMOTE(random_state=random_state)),\n",
    "                 ('standarization', StandardScaler()), \n",
    "                 ('clf', RandomForestClassifier(n_estimators=100, random_state=13))])\n",
    "#imba_pipeline = make_pipeline(SMOTE(random_state=42), RandomForestClassifier(n_estimators=100, random_state=13))\n",
    "scores=cross_val_score(pipe, X_train, y_train, scoring='roc_auc', cv=kf)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##using only randomforest with undersampling and oversampling\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "scores_under_over=pd.DataFrame()\n",
    "kf = StratifiedKFold(n_splits=5, random_state=random_state, shuffle=True)\n",
    "# for sampling in fb2.oversamplings.keys():\n",
    "#     print(sampling)\n",
    "# #fb2.models_list[1][1]\n",
    "pipe = Pipeline([('undersample', fb2.undersamplings['OSS']),\n",
    "                ('oversample', fb2.oversamplings['ROS']),\n",
    "                 ('standarization', StandardScaler()), \n",
    "                 ('clf', fb2.models_list[1][1])])\n",
    "scores=fb2.cross_validate(pipe, X_train, y_train, scoring=fb2.metrics, cv=kf)\n",
    "for key in scores_under_over:\n",
    "    scores_under_over.loc[0,key]=scores[key].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Selection so far\n",
    "#features\n",
    "numerical_features=['num_vehicles','hood_count',]\n",
    "categorical_features=['district','pedestrian','shift','weekend',]\n",
    "##model\n",
    "models_list.append(('rf',RandomForestClassifier(class_weight='balanced', max_depth=3, n_estimators=10, max_features=3, random_state=random_state))\n",
    " ##pipeline   \n",
    "RandomOverSampler, SMoteTomek,OneSidedSelection,StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1=[pred[1] for pred in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [4, 6, 10, 12],\n",
    "}\n",
    "new_params = {'clf__' + key: params[key] for key in params}\n",
    "kf = StratifiedKFold(n_splits=5, random_state=random_state, shuffle=True)\n",
    "pipe = Pipeline([('undersample', fb2.undersamplings['OSS']),\n",
    "                ('oversample', fb2.oversamplings['ROS']),\n",
    "                 ('standarization', StandardScaler()), \n",
    "                 ('clf', RandomForestClassifier(class_weight='balanced',random_state=fb2.random_state))])\n",
    "grid_imba = GridSearchCV(pipe, param_grid=new_params, cv=kf, scoring='roc_auc',\n",
    "                        return_train_score=True)\n",
    "grid_imba.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_imba.best_params_, grid_imba.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=5, random_state=random_state, shuffle=True)\n",
    "pipe = Pipeline([('undersample', fb2.undersamplings['OSS']),\n",
    "                ('oversample', fb2.oversamplings['ROS']),\n",
    "                 ('standarization', StandardScaler()), \n",
    "                 ('clf', RandomForestClassifier(class_weight='balanced',\n",
    "                                                random_state=fb2.random_state,max_depth=12,\n",
    "                                               n_estimators=100))])\n",
    "scores=fb2.cross_validate(pipe, X_train, y_train, scoring=fb2.metrics, cv=kf)\n",
    "for key in scores:\n",
    "    print(key,scores[key].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "pipe.fit(X_train,y_train)\n",
    "predictions=pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as m\n",
    "m.recall_score(y_test,predictions),m.precision_score(y_test,predictions),m.accuracy_score(y_test,predictions),m.roc_auc_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###class weight\n",
    "X_train=fb2.X_train\n",
    "y_train=fb2.y_train\n",
    "X_test=fb2.X_test\n",
    "y_test=fb2.y_test\n",
    "# numerical_features=fb2.numerical_features\n",
    "# categorical_features=fb2.categorical_features\n",
    "numerical_features=['num_vehicles','hood_count',]\n",
    "categorical_features=['district','pedestrian','shift','weekend',]\n",
    "\n",
    "\n",
    "X_train,X_test=fb2.preprocessing_fetures(X_train,X_test,numerical_features,categorical_features)\n",
    "# fb2.undersampling(X_train,y_train)\n",
    "# X_train.shape, y_train.shape\n",
    "models_list=fb2.models_list\n",
    "models=fb2.models\n",
    "random_state=fb2.random_state\n",
    "\n",
    "accuracies=[]\n",
    "recalls=[]\n",
    "precisions=[]\n",
    "aucs=[]\n",
    "for model in models_list:\n",
    "    scores = fb2.scoring_model(X_train,X_test,y_train,y_test,model[1])\n",
    "    recalls.append(scores[0])\n",
    "    precisions.append(scores[1])\n",
    "    accuracies.append(scores[2])\n",
    "    aucs.append(scores[3])\n",
    "results=pd.DataFrame()\n",
    "results['recall']=recalls\n",
    "results['precision']=precisions\n",
    "results['accuracy']=accuracies\n",
    "results['auc']=aucs\n",
    "results.index=[model[0] for model in models_list]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies=[]\n",
    "recalls=[]\n",
    "precisions=[]\n",
    "aucs=[]\n",
    "undersamplings=fb2.undersamplings\n",
    "\n",
    "for key in undersamplings.keys():\n",
    "    print(key)\n",
    "    start=time.time()\n",
    "    X_under, y_under= undersampling_dataset(X_train,y_train,model=undersamplings[key])\n",
    "    end=time.time()\n",
    "    print(key,round((end-start)/60,2),'minutes')\n",
    "    scores=fb2.scoring_model(X_under,X_test,y_under,y_test,models_dict['forest']['name'])\n",
    "    recalls.append(scores[0])\n",
    "    precisions.append(scores[1])\n",
    "    accuracies.append(scores[2])\n",
    "    aucs.append(scores[3])\n",
    "results_under=pd.DataFrame()\n",
    "results_under['recall']=recalls\n",
    "results_under['precision']=precisions\n",
    "results_under['accuracy']=accuracies\n",
    "results_under['auc']=aucs\n",
    "results_under.index=list(undersamplings.keys())\n",
    "results_under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_pickle.pkl', 'rb') as f:\n",
    "    data_pickled=pickle.load(f)\n",
    "data_pickled['results_under_w/rf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "end=tiime.time()\n",
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end=tiime.time()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Package the variables we want to export\n",
    "data = {\n",
    "    'results_under_w/rf': results_under,  # Record what was vectorized\n",
    "    'results_w/models': results,\n",
    "    'results_over': results_over,\n",
    "    \n",
    "              # For encoding additional test data\n",
    "    \n",
    "    'X_train': X_train,        # Train\n",
    "    'y_train': y_train,\n",
    "    \n",
    "    \n",
    "    }\n",
    "\n",
    "\n",
    "# 'wb' - Write + Binary File\n",
    "with open('output_pickle.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(string):\n",
    "    index = 0\n",
    "    compressed = \"\"\n",
    "    len_str = len(string)\n",
    "    while index != len_str:\n",
    "        count = 1\n",
    "        while (index < len_str-1) and (string[index] == string[index+1]):\n",
    "            count = count + 1\n",
    "            index = index + 1\n",
    "        if count == 1:\n",
    "            compressed += str(string[index])\n",
    "        else:\n",
    "            compressed += str(string[index]) + str(count)\n",
    "        index = index + 1\n",
    "    return compressed\n",
    "\n",
    "\n",
    "string= \"djjjjjjjjppjjjjjjjjjjjrhejrejj\"\n",
    "\n",
    "print(compress(string))\n",
    "\n",
    "if len(compress(string)) > len(string):\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pickled['results_uver_under_rf_pipeline']=scores_under_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pickled.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('output_pickle.pkl', 'rb') as f:\n",
    "#     data_pickled=pickle.load(f)\n",
    "# data_pickled['results_over_under']=results_over_under\n",
    "with open('output_pickle.pkl', 'wb') as f:\n",
    "    pickle.dump(data_pickled, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies=[]\n",
    "recalls=[]\n",
    "precisions=[]\n",
    "aucs=[]\n",
    "oversamplings=fb2.oversamplings\n",
    "oversampling_dataset=fb2.oversampling_dataset\n",
    "\n",
    "for key in oversamplings.keys():\n",
    "    print(key)\n",
    "    start=time.time()\n",
    "    X_under, y_under= oversampling_dataset(X_train,y_train,model=oversamplings[key])\n",
    "    end=time.time()\n",
    "    print(key,round((end-start)/60,2),'minutes')\n",
    "    scores=fb2.scoring_model(X_under,X_test,y_under,y_test,models_dict['forest']['name'])\n",
    "    recalls.append(scores[0])\n",
    "    precisions.append(scores[1])\n",
    "    accuracies.append(scores[2])\n",
    "    aucs.append(scores[3])\n",
    "results_over=pd.DataFrame()\n",
    "results_over['recall']=recalls\n",
    "results_over['precision']=precisions\n",
    "results_over['accuracy']=accuracies\n",
    "results_over['auc']=aucs\n",
    "results_over.index=list(oversamplings.keys())\n",
    "results_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pickled['results_over_w/rf'][data_pickled['results_over_w/rf']['auc']>0.58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies=[]\n",
    "recalls=[]\n",
    "precisions=[]\n",
    "aucs=[]\n",
    "over_undersamplings=fb2.over_undersamplings\n",
    "#over_undersampling_dataset=fb2.over_undersampling_dataset\n",
    "\n",
    "for key in over_undersamplings.keys():\n",
    "    print(key)\n",
    "    start=time.time()\n",
    "    X_under, y_under= oversampling_dataset(X_train,y_train,model=over_undersamplings[key])\n",
    "    end=time.time()\n",
    "    print(key,round((end-start)/60,2),'minutes')\n",
    "    scores=fb2.scoring_model(X_under,X_test,y_under,y_test,models_dict['forest']['name'])\n",
    "    recalls.append(scores[0])\n",
    "    precisions.append(scores[1])\n",
    "    accuracies.append(scores[2])\n",
    "    aucs.append(scores[3])\n",
    "results_over_under=pd.DataFrame()\n",
    "results_over_under['recall']=recalls\n",
    "results_over_under['precision']=precisions\n",
    "results_over_under['accuracy']=accuracies\n",
    "results_over_under['auc']=aucs\n",
    "results_over_under.index=list(over_undersamplings.keys())\n",
    "results_over_under"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from imblearn.pipeline import Pipeline\n",
    " \n",
    "\n",
    "# over = ...\n",
    "# under = ...\n",
    "# define pipeline\n",
    "#pipeline = Pipeline(steps=[('o', over), ('u', under)])\n",
    "#oversamplings['SVMSMOTE']\n",
    "undersamplings=fb2.undersamplings\n",
    "#undersamplings['NCL']\n",
    "###########################\n",
    "model = models_dict['forest']['name']\n",
    "# define resampling\n",
    "over = oversamplings['SVMSMOTE']\n",
    "under = undersamplings['NCL']\n",
    "# define pipeline\n",
    "pipeline = Pipeline(steps=[('o', over), ('u', under), ('m', model)])\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(pipeline, X_train, y_train, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "# summarize performance\n",
    "print('Mean ROC AUC: %.3f' % mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "pred=pipeline.predict(X_test)\n",
    "print(\"Accuracy: \",accuracy_score(y_test,pred),'\\n')\n",
    "print(\"Recall :\", recall_score(y_test,pred))\n",
    "print(\"auc: \", roc_auc_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and test data\n",
    "numerical_features=['num_vehicles','hood_count',]\n",
    "categorical_features=['district','pedestrian','shift','weekend',]\n",
    "\n",
    "accidents['hour']=accidents.hour.astype(str)\n",
    "accidents['year']=accidents.year.astype(str)\n",
    "acc_20=accidents[accidents.year=='2020']\n",
    "acc_19=accidents[accidents.year=='2019']\n",
    "test=accidents[accidents.year.isin(['2019','2020'])]\n",
    "train=accidents[~accidents.year.isin(['2019','2020'])]\n",
    "\n",
    "X_train=pd.concat([train[numerical_features],pd.get_dummies(train[categorical_features],drop_first=True)],axis=1)\n",
    "y_train=train.target\n",
    "X_test=pd.concat([test[numerical_features],pd.get_dummies(test[categorical_features],drop_first=True)],axis=1)\n",
    "y_test=test.target\n",
    "ratio_strategy=0.8\n",
    "smote = SMOTE(sampling_strategy=ratio_strategy,random_state = 99)\n",
    "X_train_over, y_train_over = smote.fit_resample(X_train, y_train)\n",
    "# Instantiate the visualizer with the classification model\n",
    "model = GradientBoostingClassifier(random_state=44)\n",
    "# visualizer = ROCAUC(model, classes=[\"no_injuries\", \"injuries\"])\n",
    "\n",
    "# visualizer.fit(X_train_over, y_train_over)        # Fit the training data to the visualizer\n",
    "# visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "# visualizer.show()                       # Finalize and show the figure\n",
    "model.fit(X_train_over,y_train_over)\n",
    "\n",
    "print('f2_score: ',fbeta_score(y_test,model.predict(X_test),beta=2))\n",
    "recall= recall_score(y_test,model.predict(X_test))\n",
    "print('Recall: ', recall)\n",
    "\n",
    "infile = open('../scores/scores.pkl','rb')\n",
    "scores_dict = pickle.load(infile)\n",
    "infile.close()\n",
    "# scores_dict={}\n",
    "# scores_dict['scores']={}\n",
    "# scores_dict['features']=[]\n",
    "# scores_dict['params']={}\n",
    "best_recall=scores_dict['recall']['score']\n",
    "#best_recall=0\n",
    "best_auc=scores_dict['auc']['score']\n",
    "#best_auc=0\n",
    "accuracy=accuracy_score(y_test,model.predict(X_test))\n",
    "print('Accuracy: ', accuracy)\n",
    "print('Cohen-Kappa :', cohen_kappa_score(y_test,model.predict(X_test)))\n",
    "auc=roc_auc_score(y_test,model.predict(X_test))\n",
    "print('AUC :', auc)\n",
    "if recall > best_recall:\n",
    "    \n",
    "    scores_dict['recall']['score']=recall\n",
    "    scores_dict['recall'].update( {'features' : numerical_features+categorical_features} )\n",
    "   \n",
    "    scores_dict['recall']['params']['smote']=ratio_strategy\n",
    "    scores_dict['recall']['params']['model']=model\n",
    "    scores_dict['recall']['params']['accuracy']=accuracy\n",
    "    best_recall=recall\n",
    "    outfile = open('../scores/scores.pkl','wb')\n",
    "    pickle.dump(scores_dict,outfile)\n",
    "    outfile.close()    \n",
    "    print('Recall improved: ', round(recall,4))\n",
    "    \n",
    "\n",
    "if auc > best_auc:\n",
    "  \n",
    "    scores_dict['auc']['score']=auc\n",
    "    scores_dict['auc'].update( {'features' : numerical_features+categorical_features} )\n",
    "   \n",
    "    scores_dict['auc']['params']['smote']=ratio_strategy\n",
    "    scores_dict['auc']['params']['model']=model\n",
    "    scores_dict['auc']['params']['accuracy']=accuracy\n",
    "    best_auc=auc\n",
    "    outfile = open('../scores/scores.pkl','wb')\n",
    "    pickle.dump(scores_dict,outfile)\n",
    "    outfile.close()\n",
    "\n",
    "    print('auc improved: ', round(auc,2))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choosing_ratio_strategy(ratio_strategy):    \n",
    "    numerical_features=['num_vehicles','hood_count',]\n",
    "    categorical_features=['district','pedestrian','shift','weekend',]\n",
    "\n",
    "    accidents['hour']=accidents.hour.astype(str)\n",
    "    accidents['year']=accidents.year.astype(str)\n",
    "    acc_20=accidents[accidents.year=='2020']\n",
    "    acc_19=accidents[accidents.year=='2019']\n",
    "    test=accidents[accidents.year.isin(['2019','2020'])]\n",
    "    train=accidents[~accidents.year.isin(['2019','2020'])]\n",
    "\n",
    "    X_train=pd.concat([train[numerical_features],pd.get_dummies(train[categorical_features],drop_first=True)],axis=1)\n",
    "    y_train=train.target\n",
    "    X_test=pd.concat([test[numerical_features],pd.get_dummies(test[categorical_features],drop_first=True)],axis=1)\n",
    "    y_test=test.target\n",
    "    print('Ratio: ', ratio_strategy)\n",
    "    smote = SMOTE(sampling_strategy=ratio_strategy,random_state = 99)\n",
    "    X_train_over, y_train_over = smote.fit_resample(X_train, y_train)\n",
    "    # Instantiate the visualizer with the classification model\n",
    "    ss=StandardScaler()\n",
    "    X_train=ss.fit_transform(X_train_over)\n",
    "    X_test=ss.transform(X_test)\n",
    "    model = GradientBoostingClassifier(random_state=44)\n",
    "    \n",
    "    model.fit(X_train, y_train_over) \n",
    "    #preds=[1 if x[1] >0.45 else 0 for x in model.predict_proba(X_test)]\n",
    "    predictions=model.predict(X_test)\n",
    "    recall= recall_score(y_test,predictions)\n",
    "    print('Recall: ', recall)\n",
    "\n",
    "    accuracy=accuracy_score(y_test,predictions)\n",
    "    print('Accuracy: ', accuracy)\n",
    "    auc=roc_auc_score(y_test,predictions)\n",
    "    print('AUC :', auc)\n",
    "for x in np.linspace(0.5,.9,5):\n",
    "    choosing_ratio_strategy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0 if value='TurboTax' else 1 for value in df.column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf=confusion_matrix(y_test,visualizer.predict(X_test))\n",
    "proba_1=[x[1] for x in model.predict_proba(X_test)]\n",
    "def threshold_clac(threshold, proba_1, y_test):\n",
    "    preds=[1 if x>threshold else 0 for x in proba_1]\n",
    "    return recall_score(y_test,preds), roc_auc_score(y_test,preds), accuracy_score(y_test,preds)\n",
    "for t in np.linspace(0.1,.5,5):\n",
    "    print(t,threshold_clac(t,proba_1,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(threshold_clac(0.45,proba_1,y_test))\n",
    "probas=[x[1] for x in visualizer.predict_proba(X_test)]\n",
    "preds=[1 if x>.45 else 0 for x in probas]\n",
    "cf=confusion_matrix(y_test,model.predict(X_test))\n",
    "pd.DataFrame(cf, columns=['pred_0','pred_1'],index=['actual_0','actual_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choosing_model(model,scoring='recall'):    \n",
    "    numerical_features=['num_vehicles','hood_count',]\n",
    "    categorical_features=['district','pedestrian','shift','weekend',]\n",
    "\n",
    "    accidents['hour']=accidents.hour.astype(str)\n",
    "    accidents['year']=accidents.year.astype(str)\n",
    "    acc_20=accidents[accidents.year=='2020']\n",
    "    acc_19=accidents[accidents.year=='2019']\n",
    "    test=accidents[accidents.year.isin(['2019','2020'])]\n",
    "    train=accidents[~accidents.year.isin(['2019','2020'])]\n",
    "\n",
    "    X_train=pd.concat([train[numerical_features],pd.get_dummies(train[categorical_features],drop_first=True)],axis=1)\n",
    "    y_train=train.target\n",
    "    X_test=pd.concat([test[numerical_features],pd.get_dummies(test[categorical_features],drop_first=True)],axis=1)\n",
    "    y_test=test.target\n",
    "    ratio_strategy=0.7\n",
    "    smote = SMOTE(sampling_strategy=ratio_strategy,random_state = 99)\n",
    "    X_train_over, y_train_over = smote.fit_resample(X_train, y_train)\n",
    "    ss=StandardScaler()\n",
    "    X_train=ss.fit_transform(X_train_over)\n",
    "    X_test=ss.transform(X_test)\n",
    "    # Instantiate the visualizer with the classification model\n",
    "    #model = GradientBoostingClassifier(random_state=44)\n",
    "    print(model)\n",
    "    scoring=scoring\n",
    "    kfold=KFold(n_splits=5, random_state=random_state,shuffle=True)\n",
    "\n",
    "    return cross_val_score(model,X_train,y_train,cv=kfold,scoring=scoring)\n",
    "    \n",
    "\n",
    "random_state=3456\n",
    "models_list=[]\n",
    "models_list.append(('logreg',LogisticRegression(C= 1, random_state=random_state)))\n",
    "models_list.append(('rf',RandomForestClassifier(class_weight='balanced', max_depth=3, n_estimators=10, max_features=3, random_state=random_state)))\n",
    "models_list.append(('lda', LinearDiscriminantAnalysis()))\n",
    "models_list.append(('linearsvc',LinearSVC(C=1.0, random_state=random_state, class_weight='balanced')))\n",
    "models_list.append(('gbc', GradientBoostingClassifier(random_state=random_state)))\n",
    "models_list.append(('XGBoost',XGBClassifier(random_state=random_state)))\n",
    "results=[]\n",
    "names=[]\n",
    "for name,model in models_list:\n",
    "    print(name)\n",
    "    results.append(choosing_model(model))\n",
    "    names.append(name)\n",
    "    #print(type(key))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies=[]\n",
    "for name in names:\n",
    "    accuracies.append(resultats[name]['accuracy'])\n",
    "\n",
    "fig=plt.figure()\n",
    "fig.suptitle('Accuracy')\n",
    "ax=fig.add_subplot(111)\n",
    "plt.boxplot(accuracies)\n",
    "ax.set_xticklabels(names)\n",
    "\n",
    "recalls=[]\n",
    "for name in names:\n",
    "    recalls.append(resultats[name]['recall'])\n",
    "\n",
    "fig=plt.figure()\n",
    "fig.suptitle('Recall')\n",
    "ax=fig.add_subplot(111)\n",
    "plt.boxplot(recalls)\n",
    "ax.set_xticklabels(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features=['num_vehicles','hood_count',]\n",
    "categorical_features=['district','pedestrian','shift','weekend',]\n",
    "\n",
    "accidents['hour']=accidents.hour.astype(str)\n",
    "accidents['year']=accidents.year.astype(str)\n",
    "acc_20=accidents[accidents.year=='2020']\n",
    "acc_19=accidents[accidents.year=='2019']\n",
    "test=accidents[accidents.year.isin(['2019','2020'])]\n",
    "train=accidents[~accidents.year.isin(['2019','2020'])]\n",
    "\n",
    "X_train=pd.concat([train[numerical_features],pd.get_dummies(train[categorical_features],drop_first=True)],axis=1)\n",
    "y_train=train.target\n",
    "X_test=pd.concat([test[numerical_features],pd.get_dummies(test[categorical_features],drop_first=True)],axis=1)\n",
    "y_test=test.target\n",
    "ratio_strategy=0.7\n",
    "smote = SMOTE(sampling_strategy=ratio_strategy,random_state = 99)\n",
    "X_train_over, y_train_over = smote.fit_resample(X_train, y_train)\n",
    "ss=StandardScaler()\n",
    "x_train=ss.fit_transform(X_train_over)\n",
    "X_test=ss.transform(X_test)\n",
    "# Instantiate the visualizer with the classification model\n",
    "model = RandomForestClassifier(class_weight='balanced', max_depth=3, n_estimators=10, max_features=3, random_state=random_state)\n",
    "#print(model)\n",
    "visualizer = ROCAUC(model, classes=[\"no_injuries\", \"injuries\"])\n",
    "visualizer.fit(X_train, y_train)\n",
    "#preds=[1 if x[1] >0.45 else 0 for x in visualizer.predict_proba(X_test)]\n",
    "\n",
    "recall= recall_score(y_test,visualizer.predict(X_test))\n",
    "print('Recall: ', recall)\n",
    "\n",
    "accuracy=accuracy_score(y_test,preds)\n",
    "print('Accuracy: ', accuracy)\n",
    "auc=roc_auc_score(y_test,preds)\n",
    "print('AUC :', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf=confusion_matrix(y_test,visualizer.predict(X_test))\n",
    "proba_1=[x[1] for x in visualizer.predict_proba(X_test)]\n",
    "def threshold_clac(threshold, proba_1, y_test):\n",
    "    preds=[1 if x>threshold else 0 for x in proba_1]\n",
    "    return recall_score(y_test,preds), roc_auc_score(y_test,preds), accuracy_score(y_test,preds)\n",
    "for t in np.linspace(0.1,.5,5):\n",
    "    print(t,threshold_clac(t,proba_1,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=visualizer.predict(X_test)\n",
    "tp=0\n",
    "tn=0\n",
    "fp=0\n",
    "fn=0\n",
    "conf_matrix=pd.Series(['tp' if (x[0]==x[1]) and (x[0]==1) else 'tn' if x[0]==x[1] else 'fn' if x[0]==1 and x[1]==0 else 'fp' for x in zip(y_test,preds)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negatives=X_test.iloc[conf_matrix[conf_matrix=='fn'].index]\n",
    "(false_negatives[[x for x in false_negatives.columns if 'district' in x]].sum()).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_negatives.groupby('district')['neighborhood'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=accidents.drop('target',axis=1).copy()\n",
    "numerical_f=accidents.select_dtypes(exclude='object').columns\n",
    "categorical_f=accidents.select_dtypes(include='object').columns\n",
    "X=pd.concat([accidents[numerical_f],pd.get_dummies(accidents[categorical_f],drop_first=True)],axis=1)\n",
    "y=accidents.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ExtraTreesClassifier(n_estimators=100)\n",
    "model.fit(X,y)\n",
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fimp=model.feature_importances_\n",
    "len(fimp), X.shape\n",
    "x=dict(zip(X.columns, fimp))\n",
    "sorted(x.items(), key=lambda item: item[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###FUNCTIONS\n",
    "\n",
    "\n",
    "def sorting_by_corr(dataframe, features, target):\n",
    "    \n",
    "    \"\"\"Sorting all features by its correlation with target\"\"\"\n",
    "    \n",
    "    \n",
    "    data = dataframe[features].copy()\n",
    "    data.reset_index(inplace=True)\n",
    "    nova_data = pd.DataFrame()\n",
    "    \n",
    "    for item in data[features].select_dtypes(exclude=['int', 'float']).columns:\n",
    "            \n",
    "        prova = pd.get_dummies(data[item])\n",
    "        nova_data = pd.concat([nova_data,prova], axis=1)\n",
    "        \n",
    "    for item in data[features].select_dtypes(include=['int', 'float']).columns:\n",
    "        nova_data[item] = data[item]\n",
    "    nova_data[target] = dataframe[target]    \n",
    "     \n",
    "    correlation = nova_data.corr()\n",
    "    \n",
    "    \n",
    "    correlation[target] = [x if x >= 0 else (-1) * x for x in correlation[target]]\n",
    "            \n",
    "    \n",
    "    all_features = list(correlation[target].sort_values(ascending=False)[1::].index)\n",
    "    final_df = nova_data[all_features]\n",
    "    return all_features, final_df\n",
    "\n",
    "\n",
    "def changing_threshold(proba, threshold_majority):\n",
    "    \n",
    "    \"\"\"recalculating predictions based on threshold\"\"\"\n",
    "    \n",
    "    return [0 if x[0] >= threshold_majority else 1 for x in proba]\n",
    "\n",
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "            TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "            FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "            TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "            FN += 1\n",
    "\n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "\n",
    "def selecting_features(dataframe, target, features):\n",
    "    \n",
    "    \"\"\"Function to select features through combinationa\"\"\"\n",
    "    \n",
    "    data = dataframe[features].copy()\n",
    "    data.reset_index(inplace=True)\n",
    "    for item in features:\n",
    "        if data[item].dtypes == 'object':\n",
    "            #print(item, data[item].dtypes, 'object')\n",
    "            prova = pd.get_dummies(data[item], drop_first=True)\n",
    "            data = pd.concat([data,prova], axis=1)\n",
    "            data.drop(item, axis=1, inplace=True)\n",
    "\n",
    "    X = np.array(data)\n",
    "    y = dataframe[target]\n",
    "    y = (np.where (y > 0, 1, 0))\n",
    "    y = np.reshape(y, len(y))\n",
    "    \n",
    "    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=1, stratify=y)\n",
    "    \n",
    "    \n",
    "    X_resampled, y_resampled = SMOTE(ratio=0.3, random_state=2018).fit_sample(X_train, y_train)\n",
    "\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(X_resampled)\n",
    "    X_train_std = ss.transform(X_resampled)\n",
    "    X_test_std = ss.transform(X_test)\n",
    "    \n",
    "    \n",
    "    clf = LogisticRegression()        \n",
    "    clf.fit(X_train_std, y_resampled)\n",
    "    score = clf.score(X_test_std, y_test)\n",
    "    pred = clf.predict(X_test_std)\n",
    "    prob = clf.predict_proba(X_test_std)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    fbeta = fbeta_score(y_test, pred, beta=0.5)\n",
    "    \n",
    "    return accuracy, recall, precision, f1, fbeta, prob, y_test\n",
    "\n",
    "\n",
    "def selecting_features_corr(dataframe, target, features):\n",
    "    \n",
    "    \"\"\"It is a similar function thatn the one with combinations. In this case, features\n",
    "    are already separated and dummied\"\"\"\n",
    "    \n",
    "    data = dataframe[features].copy()\n",
    "    data.reset_index(inplace=True)\n",
    "\n",
    "    X = np.array(data)\n",
    "    y = dataframe[target]\n",
    "    y = (np.where (y > 0, 1, 0))\n",
    "    y = np.reshape(y, len(y))\n",
    "    \n",
    "    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=1, stratify=y)\n",
    "    \n",
    "    \n",
    "    X_resampled, y_resampled = SMOTE(ratio=0.3, random_state=2018).fit_sample(X_train, y_train)\n",
    "\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(X_resampled)\n",
    "    X_train_std = ss.transform(X_resampled)\n",
    "    X_test_std = ss.transform(X_test)\n",
    "    \n",
    "    \n",
    "    clf = LogisticRegression()        \n",
    "    clf.fit(X_train_std, y_resampled)\n",
    "    score = clf.score(X_test_std, y_test)\n",
    "    pred = clf.predict(X_test_std)\n",
    "    prob = clf.predict_proba(X_test_std)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    fbeta = fbeta_score(y_test, pred, beta=0.5)\n",
    "    \n",
    "    return accuracy, recall, precision, f1, fbeta, prob, pred\n",
    "\n",
    "### Pickling resultat\n",
    "\n",
    "def pickling_resultat(resultat):\n",
    "    \n",
    "    for i in resultat.keys():\n",
    "        with open('resultat{}.pkl'.format(i), 'wb') as picklefile:\n",
    "            pickle.dump(resultat[i], picklefile)\n",
    "            \n",
    "def depickling_resultat(amount_features):\n",
    "    resultat = {}\n",
    "    for i in range (1, amount_features + 1):\n",
    "        \n",
    "        with open('resultat{}.pkl'.format(i), 'rb') as f:\n",
    "            resultat[i] = pickle.load(f)\n",
    "    return resultat\n",
    "\n",
    "def selecting_model(dataframe, target, features, model):\n",
    "    \n",
    "    data = dataframe[features].copy()\n",
    "    data.reset_index(inplace=True)\n",
    "    for item in features:\n",
    "        if data[item].dtypes == 'object':\n",
    "            #print(item, data[item].dtypes, 'object')\n",
    "            prova = pd.get_dummies(data[item], drop_first=True)\n",
    "            data = pd.concat([data,prova], axis=1)\n",
    "            data.drop(item, axis=1, inplace=True)\n",
    "\n",
    "    X = np.array(data)\n",
    "    y = dataframe[target]\n",
    "    y = (np.where (y > 0, 1, 0))\n",
    "    y = np.reshape(y, len(y))\n",
    "    \n",
    "    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=1, stratify=y)\n",
    "    \n",
    "    \n",
    "    X_resampled, y_resampled = SMOTE(ratio=0.3, random_state=2018).fit_sample(X_train, y_train)\n",
    "\n",
    "    #print('Labels counts in y:', np.bincount(y), (np.bincount(y)[1]/np.bincount(y)[0]))\n",
    "    #print('Labels counts in y_train:', np.bincount(y_train), (np.bincount(y_train)[1]/np.bincount(y_train)[0]))\n",
    "    #print('Labels counts in y_test:', np.bincount(y_test), (np.bincount(y_test)[1]/np.bincount(y_test)[0]))\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(X_resampled)\n",
    "    X_train_std = ss.transform(X_resampled)\n",
    "    X_test_std = ss.transform(X_test)\n",
    "    \n",
    "\n",
    "                \n",
    "    model.fit(X_train_std, y_resampled)\n",
    "    pred = model.predict(X_test_std)\n",
    "    #prob = clf.predict_proba(X_test_std)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    f1 = f1_score(y_test, pred)\n",
    "        \n",
    "    return accuracy, recall, precision, f1, pred, y_test\n",
    "    \n",
    "    \n",
    "def preprocessing_features(df, features):\n",
    "    \n",
    "    \"Getting ready the features for the model\"\n",
    "    \n",
    "    data = df[features].copy()\n",
    "    data.reset_index(inplace=True)\n",
    "    for item in features:\n",
    "        if data[item].dtypes == 'object':\n",
    "            #print(item, data[item].dtypes)\n",
    "            prova = pd.get_dummies(data[item], drop_first=True)\n",
    "            data = pd.concat([data,prova], axis=1)\n",
    "            data.drop(item, axis=1, inplace=True)\n",
    "\n",
    "    features_array = np.array(data)\n",
    "    \n",
    "    return features_array    \n",
    "\n",
    "\n",
    "def selecting_model_oversampling(train,test, features, model):\n",
    "    \n",
    "    train.majority = train[train.victims_alert == 0]\n",
    "    train.minority = train[train.victims_alert == 1]\n",
    "\n",
    "    train_minority_upsampled = resample(train_minority, \n",
    "                                     replace=True,     # sample with replacement\n",
    "                                     n_samples=len(train_majority),    # to match majority class\n",
    "                                     random_state=1) # reproducible results\n",
    "\n",
    "    train_upsampled = pd.concat([train_majority, train_minority_upsampled])\n",
    "\n",
    "    y = train_upsampled.victims_alert\n",
    "    X = preprocessing_features(train_upsampled, features)\n",
    "    y_test = test.victims_alert\n",
    "    X_test = preprocessing_features(test, features)\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(X)\n",
    "    X_train_std = ss.transform(X)\n",
    "    X_test_std = ss.transform(X_test)       \n",
    "    clf = model.fit(X_train_std, y)\n",
    "    pred = clf.predict(X_test_std)\n",
    "    \n",
    "    return pred, y_test\n",
    "    \n",
    "    \n",
    "def selecting_model_downsampling(train,test, features, model):\n",
    "    \n",
    "    train.majority = train[train.victims_alert == 0]\n",
    "    train.minority = train[train.victims_alert == 1]\n",
    "\n",
    "    train_majority_downsampled = resample(train_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=len(train_minority),     # to match minority class\n",
    "                                 random_state=1) # reproducible results\n",
    "\n",
    "    train_downsampled = pd.concat([train_minority, train_majority_downsampled])\n",
    "    \n",
    "    y = train_downsampled.victims_alert\n",
    "    X = preprocessing_features(train_downsampled, features)\n",
    "    y_test = test.victims_alert\n",
    "    X_test = preprocessing_features(test, features)\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(X)\n",
    "    X_train_std = ss.transform(X)\n",
    "    X_test_std = ss.transform(X_test)       \n",
    "    clf = model.fit(X_train_std, y)\n",
    "    pred = clf.predict(X_test_std)\n",
    "    \n",
    "    return pred, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Values to keep all the time\n",
    "metrics = ['recall', 'precision', 'f1_score', 'roc_auc', 'accuracy']\n",
    "random_state = 1\n",
    "baseline = 1 - df['victims_alert'].sum()/df.shape[0]\n",
    "\n",
    "features = []\n",
    "pickle_files = ['accuracy_features.pkl', 'f1_features.pkl',\\\n",
    "                'precision_features.pkl', 'recall_features.pkl']\n",
    "\n",
    "dict_features = {}\n",
    "count = 0\n",
    "for item in pickle_files:\n",
    "    \n",
    "    with open(item, 'rb') as f:\n",
    "        list_features = pickle.load(f)\n",
    "        count = count + len(list_features)\n",
    "        dict_features[item[0:-4]] = count\n",
    "        dict_features[item[0:-4] + \"_list\"] = list_features\n",
    "\n",
    "    for i in list_features:\n",
    "        features.append(i)\n",
    "dict_features.keys(), metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {'log_reg': {'name': LogisticRegression(C= 100, random_state=random_state)},\n",
    "'forest':{'name': RandomForestClassifier(class_weight='balanced', max_depth=5, n_estimators=10, max_features=1, random_state=random_state)}, \n",
    "'gbc': {'name': GradientBoostingClassifier(random_state=random_state, learning_rate=0.01)},\n",
    "       'linearsvc':{'name': LinearSVC(C=1.0, random_state=random_state, class_weight='balanced')},\\\n",
    "              'Perceptron': {'name': Perceptron(class_weight='balanced', random_state=random_state)},\n",
    "              'MLPClassifier': {'name': MLPClassifier(random_state=random_state)}}\n",
    "for i in models_dict.keys():\n",
    "    print(models_dict[i]['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Try with different models\n",
    "metrics = ['recall', 'precision', 'f1_score', 'roc_auc', 'accuracy']\n",
    "for key in models_dict.keys():\n",
    "    models_dict[key].update({key: None for key in metrics}) \n",
    "\n",
    "\n",
    "for key in models_dict.keys():\n",
    "    \n",
    "    accuracies = []\n",
    "    recalls =[]\n",
    "    precisions = []\n",
    "    f1s = []\n",
    "    roc_aucs = []\n",
    "    count = 0\n",
    "    for i in features:\n",
    "        \n",
    "        count = count + 1\n",
    "        a, r, p, f, pred, y_test = selecting_model(df,target, i, models_dict[key]['name'])\n",
    "        \n",
    "        accuracies.append(a)\n",
    "        recalls.append(r)\n",
    "        precisions.append(p)\n",
    "        f1s.append(f)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_test, pred)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        roc_aucs.append(roc_auc)\n",
    "        if count%10 == 0:\n",
    "            print(count)\n",
    "        \n",
    "    models_dict[key]['accuracy'] = accuracies\n",
    "    models_dict[key]['recall'] = recalls\n",
    "    models_dict[key]['precision'] = precisions\n",
    "    models_dict[key]['f1_score'] = f1s\n",
    "    models_dict[key]['roc_auc'] = roc_aucs\n",
    "    print(key)\n",
    "with open('dictionary_models.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(models_dict, picklefile)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dictionary_models.pkl', 'rb') as f:\n",
    "    models_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_models_and_metrics(n_rows, n_columns, name_metric):\n",
    "\n",
    "    fig, ax = plt.subplots(n_rows,n_columns, figsize=(10,10))\n",
    "    x_axis = [x for x in range(1, len(models_dict[key][name_metric]) + 1)]\n",
    "    models = [y for y in models_dict.keys()]\n",
    "    count = 0\n",
    "\n",
    "    for i in range(3):\n",
    "        for j in range(2):\n",
    "\n",
    "            ax[i, j].plot(x_axis, models_dict[models[count]][name_metric])\n",
    "            ax[i, j].legend([models[count]])\n",
    "            count = count + 1\n",
    "    fig.suptitle(name_metric.title(), fontsize=15); # or plt.suptitle('Main title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name_metric in metrics:\n",
    "    n_rows = 3\n",
    "    n_columns = 2\n",
    "    #name_metric = 'accuracy'\n",
    "\n",
    "    plotting_models_and_metrics(n_rows, n_columns, name_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in models_dict.keys():\n",
    "    \n",
    "    x_axis = [x for x in range(1, len(models_dict[key]['recall']) + 1)]\n",
    "    plt.plot(x_axis,models_dict[key]['recall']);\n",
    "    plt.title('Recall')\n",
    "plt.legend(models_dict.keys());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in models_dict.keys():\n",
    "    \n",
    "    x_axis = [x for x in range(1, len(models_dict[key]['precision']) + 1)]\n",
    "    plt.plot(x_axis,models_dict[key]['precision']);\n",
    "    plt.title('Precision')\n",
    "plt.legend(models_dict.keys());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in models_dict.keys():\n",
    "    \n",
    "    x_axis = [x for x in range(1, len(models_dict[key]['f1_score']) + 1)]\n",
    "    plt.plot(x_axis,models_dict[key]['f1_score']);\n",
    "    plt.title('F1 Score')\n",
    "plt.legend(models_dict.keys());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in models_dict.keys():\n",
    "    \n",
    "    x_axis = [x for x in range(1, len(models_dict[key]['roc_auc']) + 1)]\n",
    "    plt.plot(x_axis,models_dict[key]['roc_auc']);\n",
    "    plt.title('Roc auc')\n",
    "plt.legend(models_dict.keys());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see the high per metric\n",
    "maxims = {}\n",
    "\n",
    "for item in metrics:\n",
    "    maxims[item] = [0, '']\n",
    "    \n",
    "    \n",
    "for key in models_dict.keys():\n",
    "    \n",
    "    print('Accuracy: ', key,max(models_dict[key]['accuracy']))\n",
    "    print(\"Recall: \", key, max(models_dict[key]['recall']))\n",
    "    for metric in metrics:\n",
    "        \n",
    "        if max(models_dict[key][metric]) >= maxims[metric][0]:\n",
    "            maxims[metric][0] = round(max(models_dict[key][metric]),6)\n",
    "            maxims[metric][1] = key\n",
    "    \n",
    "maxims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models_dict[maxims['roc_auc'][1]]['roc_auc'] = [round(x,6) for x in models_dict[maxims['roc_auc'][1]]['roc_auc']]\n",
    "fea_index = models_dict[maxims['roc_auc'][1]]['roc_auc'].index(maxims['roc_auc'][0])\n",
    "\n",
    "a, r, p, f1, pred, y_test = selecting_model(df, target, features[fea_index], models_dict[maxims['roc_auc'][1]]['name'])\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Analyzing best Roc_auc\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "fn\n",
    "\n",
    "from sklearn.metrics import auc, precision_recall_curve, average_precision_score\n",
    "fpr, tpr, _ = roc_curve(y_test, pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area ={})'.format(round(roc_auc, 4)))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "print(\"Flase Negatives: \", fn)\n",
    "print(\"Amount of ones predicted: \", pred.sum()/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run it the first time only\n",
    "msk = np.random.rand(len(df)) < 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Oversampling minority\n",
    "\n",
    "with open('mask.pkl', 'rb') as f:\n",
    "    msk = pickle.load(f)\n",
    "\n",
    "#msk = np.random.rand(len(df)) < 0.75\n",
    "train = df[msk]\n",
    "test = df[~msk]\n",
    "    \n",
    "\n",
    "train_majority = train[train.victims_alert==0]\n",
    "\n",
    "train_minority = train[train.victims_alert==1]\n",
    " \n",
    "# Upsample minority class\n",
    "train_minority_upsampled = resample(train_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=len(train_majority),    # to match majority class\n",
    "                                 random_state=1) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "train_upsampled = pd.concat([train_majority, train_minority_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "train_upsampled.victims_alert.value_counts()\n",
    "\n",
    "\"\"\"y = train_upsampled.victims_alert\n",
    "X = preprocessing_features(train_upsampled, features[10])\n",
    "y_test = test.victims_alert\n",
    "X_test = preprocessing_features(test, features[10]) \"\"\"\n",
    "models_dict_oversampling = {key: None for key in models_dict.keys()}\n",
    "\n",
    "\n",
    "for key in models_dict_oversampling.keys():\n",
    "    models_dict_oversampling[key] = {key: None for key in metrics}\n",
    "\n",
    "\n",
    "\n",
    "for key in models_dict.keys():\n",
    "    \n",
    "    print(key)\n",
    "    accuracies = []\n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    f1s = []\n",
    "    roc_aucs = []\n",
    "    count = 0\n",
    "    \n",
    "    for i in features:\n",
    "        \n",
    "        count = count + 1\n",
    "        y = train_upsampled.victims_alert\n",
    "        X = preprocessing_features(train_upsampled, i)\n",
    "        y_test = test.victims_alert\n",
    "        X_test = preprocessing_features(test, i)\n",
    "        ss = StandardScaler()\n",
    "        ss.fit(X)\n",
    "        X_train_std = ss.transform(X)\n",
    "        X_test_std = ss.transform(X_test)          \n",
    "        clf = models_dict[key]['name'].fit(X_train_std, y)\n",
    "        pred = clf.predict(X_test_std)\n",
    "        accuracy = accuracy_score(y_test, pred)\n",
    "        recall = recall_score(y_test,pred)\n",
    "        f1 = f1_score(y_test, pred)\n",
    "        precision = precision_score(y_test,pred)\n",
    "        accuracies.append(accuracy)\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        f1s.append(f1)\n",
    "        fpr, tpr, _ = roc_curve(y_test, pred)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        roc_aucs.append(roc_auc)\n",
    "        if count%10 == 0:\n",
    "            print(count)\n",
    "        \n",
    "    models_dict_oversampling[key]['accuracy'] = accuracies\n",
    "    models_dict_oversampling[key]['recall'] = recalls\n",
    "    models_dict_oversampling[key]['precision'] = precisions\n",
    "    models_dict_oversampling[key]['f1_score'] = f1s\n",
    "    models_dict_oversampling[key]['roc_auc'] = roc_aucs\n",
    "with open('mask.pkl', 'wb') as f:\n",
    "    pickle.dump(msk, f)\n",
    "with open('oversampling_models.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(models_dict_oversampling, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('oversampling_models.pkl', 'rb') as picklefile:\n",
    "    models_dict_oversampling = pickle.load(picklefile)\n",
    "models_dict_oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxims_oversampling = {}\n",
    "for item in metrics:\n",
    "    maxims_oversampling[item] = [0, '']\n",
    "    \n",
    "    \n",
    "for key in models_dict_oversampling.keys():\n",
    "    \n",
    "    print('Accuracy: ', key,max(models_dict_oversampling[key]['accuracy']))\n",
    "    print(\"Recall: \", key, max(models_dict_oversampling[key]['recall']))\n",
    "    for metric in metrics:\n",
    "        \n",
    "        if metric != 'recall':\n",
    "        \n",
    "            if max(models_dict_oversampling[key][metric]) >= maxims_oversampling[metric][0]:\n",
    "                maxims_oversampling[metric][0] = round(max(models_dict_oversampling[key][metric]),6)\n",
    "                maxims_oversampling[metric][1] = key\n",
    "        else:\n",
    "            if (max(models_dict_oversampling[key][metric]) >= maxims_oversampling[metric][0]) and (max(models_dict_oversampling[key][metric]) < 1):\n",
    "                maxims_oversampling[metric][0] = round(max(models_dict_oversampling[key][metric]),6)\n",
    "                maxims_oversampling[metric][1] = key\n",
    "maxims_oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Analyzingthe best roc_auc\n",
    "\n",
    "models_dict_oversampling[maxims_oversampling['roc_auc'][1]]['roc_auc'] = [round(x,6) for x in models_dict_oversampling[maxims_oversampling['roc_auc'][1]]['roc_auc']]\n",
    "fea_index = models_dict_oversampling[maxims_oversampling['roc_auc'][1]]['roc_auc'].index(maxims_oversampling['roc_auc'][0])\n",
    "\n",
    "pred, y_test = selecting_model_oversampling(train, test, features[fea_index], models_dict[maxims_oversampling['roc_auc'][1]]['name'])\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"Recall: \", recall_score(y_test, pred),  \"ROC-AUC: \", roc_auc)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area ={})'.format(round(roc_auc, 4)))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "print(\"Flase Negatives: \", fn)\n",
    "print(\"Amount of ones predicted: \", pred.sum()/len(y_test))\n",
    "print(\"Features: \", features[fea_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Analyzingthe best recall\n",
    "to_consider = 'recall'\n",
    "models_dict_oversampling[maxims_oversampling[to_consider][1]][to_consider] = [round(x,6) for x in models_dict_oversampling[maxims_oversampling[to_consider][1]][to_consider]]\n",
    "fea_index = models_dict_oversampling[maxims_oversampling[to_consider][1]][to_consider].index(maxims_oversampling[to_consider][0])\n",
    "\n",
    "pred, y_test = selecting_model_oversampling(train, test, features[fea_index], models_dict[maxims_oversampling[to_consider][1]]['name'])\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"Recall: \", recall_score(y_test, pred),  \"ROC-AUC: \", roc_auc)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area ={})'.format(round(roc_auc, 4)))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "print(\"Flase Negatives: \", fn)\n",
    "print(\"Amount of ones predicted: \", pred.sum()/len(y_test))\n",
    "print(\"Features: \", features[fea_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Downsampling\n",
    "\n",
    "train_majority_downsampled = resample(train_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=len(train_minority),     # to match minority class\n",
    "                                 random_state=1) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "train_downsampled = pd.concat([train_majority_downsampled, train_minority])\n",
    " \n",
    "# Display new class counts\n",
    "\n",
    "models_dict_downsampling = {key: None for key in models_dict.keys()}\n",
    "\n",
    "\n",
    "for key in models_dict_downsampling.keys():\n",
    "    models_dict_downsampling[key] = {key: None for key in metrics}\n",
    "\n",
    "\n",
    "\n",
    "for key in models_dict.keys():\n",
    "    \n",
    "    print(key)\n",
    "    accuracies = []\n",
    "    recalls =[]\n",
    "    precisions = []\n",
    "    f1s = []\n",
    "    roc_aucs = []\n",
    "    count = 0\n",
    "    \n",
    "    for i in features:\n",
    "        \n",
    "        count = count + 1\n",
    "        y = train_downsampled.victims_alert\n",
    "        X = preprocessing_features(train_downsampled, i)\n",
    "        y_test = test.victims_alert\n",
    "        X_test = preprocessing_features(test, i)\n",
    "        ss = StandardScaler()\n",
    "        ss.fit(X)\n",
    "        X_train_std = ss.transform(X)\n",
    "        X_test_std = ss.transform(X_test)          \n",
    "        clf = models_dict[key]['name'].fit(X_train_std, y)\n",
    "        pred = clf.predict(X_test_std)\n",
    "        accuracy = accuracy_score(y_test, pred)\n",
    "        recall = recall_score(y_test,pred)\n",
    "        f1 = f1_score(y_test, pred)\n",
    "        precision = precision_score(y_test,pred)\n",
    "        accuracies.append(accuracy)\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        f1s.append(f1)\n",
    "        fpr, tpr, _ = roc_curve(y_test, pred)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        roc_aucs.append(roc_auc)\n",
    "        if count%10 == 0:\n",
    "            print(count)\n",
    "        \n",
    "        \n",
    "    models_dict_downsampling[key]['accuracy'] = accuracies\n",
    "    models_dict_downsampling[key]['recall'] = recalls\n",
    "    models_dict_downsampling[key]['precision'] = precisions\n",
    "    models_dict_downsampling[key]['f1_score'] = f1s\n",
    "    models_dict_downsampling[key]['roc_auc'] = roc_aucs\n",
    "    \n",
    "with open('downsampling_models.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(models_dict_downsampling, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('downsampling_models.pkl', 'rb') as picklefile:\n",
    "    models_dict_downsampling = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxims_downsampling = {}\n",
    "\n",
    "for item in metrics:\n",
    "    maxims_downsampling[item] = [0, '']\n",
    "    \n",
    "    \n",
    "for key in models_dict_downsampling.keys():\n",
    "    \n",
    "    print('Accuracy: ', key,max(models_dict_downsampling[key]['accuracy']))\n",
    "    print(\"Recall: \", key, max(models_dict_downsampling[key]['recall']))\n",
    "    for metric in metrics:\n",
    "        \n",
    "        if max(models_dict_downsampling[key][metric]) >= maxims_downsampling[metric][0]:\n",
    "            maxims_downsampling[metric][0] = round(max(models_dict_downsampling[key][metric]),6)\n",
    "            maxims_downsampling[metric][1] = key\n",
    "    \n",
    "maxims_downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Analyzingthe best roc_auc\n",
    "to_consider = 'roc_auc'\n",
    "models_dict_downsampling[maxims_downsampling[to_consider][1]][to_consider] = [round(x,6) for x in models_dict_downsampling[maxims_downsampling[to_consider][1]][to_consider]]\n",
    "fea_index = models_dict_downsampling[maxims_downsampling[to_consider][1]][to_consider].index(maxims_downsampling[to_consider][0])\n",
    "\n",
    "pred, y_test = selecting_model_downsampling(train, test, features[fea_index], models_dict[maxims_downsampling[to_consider][1]]['name'])\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"Recall: \", recall_score(y_test, pred),  \"ROC-AUC: \", roc_auc)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area ={})'.format(round(roc_auc, 4)))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "print(\"Flase Negatives: \", fn)\n",
    "print(\"Amount of ones predicted: \", pred.sum()/len(y_test))\n",
    "print(\"Features: \", features[fea_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Analyzingthe best recall\n",
    "to_consider = 'recall'\n",
    "\n",
    "models_dict_downsampling[maxims_downsampling[to_consider][1]][to_consider] = [round(x,6) for x in models_dict_downsampling[maxims_downsampling[to_consider][1]][to_consider]]\n",
    "fea_index = models_dict_downsampling[maxims_downsampling[to_consider][1]][to_consider].index(maxims_downsampling[to_consider][0])\n",
    "\n",
    "pred, y_test = selecting_model_downsampling(train, test, features[fea_index], models_dict[maxims_downsampling[to_consider][1]]['name'])\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"Recall: \", recall_score(y_test, pred),  \"ROC-AUC: \", roc_auc)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area ={})'.format(round(roc_auc, 4)))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "print(\"Flase Negatives: \", fn)\n",
    "print(\"Amount of ones predicted: \", pred.sum()/len(y_test))\n",
    "print(\"Features: \", features[fea_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.fixes import signature\n",
    "average_precision = average_precision_score(y_test, pred)\n",
    "precision, recall, _ = precision_recall_curve(y_test, pred)\n",
    "\n",
    "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "step_kwargs = ({'step': 'post'}\n",
    "               if 'step' in signature(plt.fill_between).parameters\n",
    "               else {})\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Sensitive Analysis\n",
    "## Next steps: add smote to costcla\n",
    "### try Bayes method\n",
    "### try QuadraticDiscriminantAnalysis and Fuzzy SVM\n",
    "### Introduce cross validationa as mentioned in a youtube video\n",
    "### class_weight is your friend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = pd.DataFrame(pd.Series(df.victims_alert).value_counts())\n",
    "obj['Percentage'] = obj['victims_alert']/obj['victims_alert'].sum()\n",
    "obj.index = ['Negative (No deaths nor severely injured)', 'Positive (Deaths and/or severely injured)']\n",
    "obj.rename(columns = {'victims_alert':'Frequency'}, inplace=True)\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prova_features = preprocessing_features(df, features[40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing_X_and_y(dataframe, features, target):\n",
    "    \n",
    "    \n",
    "    data = dataframe[features].copy()\n",
    "    data.reset_index(inplace=True)\n",
    "    for item in features:\n",
    "        if data[item].dtypes == 'object':\n",
    "            prova = pd.get_dummies(data[item], drop_first=True)\n",
    "            data = pd.concat([data,prova], axis=1)\n",
    "            data.drop(item, axis=1, inplace=True)\n",
    "            \n",
    "    X = np.array(data)\n",
    "    y = dataframe[target]\n",
    "    y = (np.where (y > 0, 1, 0))\n",
    "    y = np.reshape(y, len(y))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=1, stratify=y)\n",
    "    \n",
    "    \n",
    "    X_resampled, y_resampled = SMOTE(ratio=0.3, random_state=2018).fit_sample(X_train, y_train)\n",
    "\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(X_resampled)\n",
    "    X_train_std = ss.transform(X_resampled)\n",
    "    X_test_std = ss.transform(X_test)\n",
    "    \n",
    "    return X_train_std, X_test_std, y_resampled, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = preparing_X_and_y(df, features[37], target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_mat[:, 0] = false_positive_cost\n",
    "cost_mat[:, 1] = false_negative_cost\n",
    "cost_mat[:, 2] = true_positive_cost\n",
    "cost_mat[:, 3] = true_negative_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FP, FN, TP, TN\n",
    "costs = [1,20, -1, 0]\n",
    "cost_mat = np.zeros((df.shape[0], 4))\n",
    "cost_mat_train = np.zeros((X_train.shape[0], 4))\n",
    "cost_mat_test = np.zeros((X_test.shape[0], 4))\n",
    "for i in range(0,4):\n",
    "    cost_mat[:, i] = costs[i] ##FP\n",
    "    cost_mat_train[:, i] = costs[i]    \n",
    "    cost_mat_test[:, i] = costs[i]     \n",
    "###Cost all zeros\n",
    "cost_all_zeros = cost_loss(y_test, np.zeros(len(y_test)), cost_mat_test)\n",
    "###\n",
    "cost_all_ones = cost_loss(y_test, np.ones(len(y_test)), cost_mat_test)\n",
    "cost_to_beat = min(cost_all_ones, cost_all_zeros)\n",
    "print(\"all zeros: \", cost_all_zeros)\n",
    "print(\"all ones: \", cost_all_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict_cs = {'log_reg': {'F': LogisticRegression(C= 100, random_state=random_state)},\n",
    "'forest':{'F': RandomForestClassifier(class_weight='balanced', max_depth=5, n_estimators=10, max_features=1, random_state=random_state)}, \n",
    "'gbc': {'F': GradientBoostingClassifier(random_state=random_state, learning_rate=0.01)},\n",
    "       'linearsvc':{'F': LinearSVC(C=1.0, random_state=random_state, class_weight='balanced')},\\\n",
    "              'Perceptron': {'F': Perceptron(class_weight='balanced', random_state=random_state)},\n",
    "              'MLPClassifier': {'F': MLPClassifier(random_state=random_state)}}\n",
    "measures = {\"f1_score\": f1_score, \"precision\": precision_score, \n",
    "            \"recall\": recall_score, \"accuracy\": accuracy_score, 'roc_auc': roc_auc_score,\\\n",
    "           'savings': savings_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding savings as a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultats = pd.DataFrame(columns=measures.keys())\n",
    "\n",
    "for key in models_dict_cs.keys():\n",
    "    \n",
    "    print(key)\n",
    "    accuracies = []\n",
    "    recalls =[]\n",
    "    precisions = []\n",
    "    f1s = []\n",
    "    roc_aucs = []\n",
    "    savings = []\n",
    "    \n",
    "    \n",
    "    for i in features:\n",
    "\n",
    "        X_train, X_test, y_train, y_test = preparing_X_and_y(df, i, target)\n",
    "    \n",
    "        clf = models_dict_cs[key]['F'].fit(X_train, y_train)\n",
    "    \n",
    "        pred = clf.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, pred)\n",
    "        recall = recall_score(y_test,pred)\n",
    "        f1 = f1_score(y_test, pred)\n",
    "        precision = precision_score(y_test,pred)\n",
    "        accuracies.append(accuracy)\n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        f1s.append(f1)\n",
    "        fpr, tpr, _ = roc_curve(y_test, pred)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        roc_aucs.append(roc_auc)\n",
    "        ###define clas_mat_test\n",
    "        \n",
    "        cost_mat_test = np.zeros((X_test.shape[0], 4))\n",
    "        for i in range(0,4):\n",
    "            cost_mat_test[:, i] = costs[i]   \n",
    "        saving = savings_score(y_test, pred, cost_mat_test)\n",
    "        savings.append(saving)\n",
    "    \n",
    "        \n",
    "    models_dict_cs[key]['accuracy'] = accuracies\n",
    "    models_dict_cs[key]['recall'] = recalls\n",
    "    models_dict_cs[key]['precision'] = precisions\n",
    "    models_dict_cs[key]['f1_score'] = f1s\n",
    "    models_dict_cs[key]['roc_auc'] = roc_aucs\n",
    "    models_dict_cs[key]['savings'] = savings\n",
    "    \n",
    "with open('cs_models.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(models_dict_cs, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cs_models.pkl', 'rb') as picklefile:\n",
    "    models_dict_cs = pickle.load(picklefile)\n",
    "    \n",
    "models_dict_cs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxims_cs = {}\n",
    "\n",
    "for item in measures:\n",
    "    maxims_cs[item] = [0, ['']]\n",
    "    \n",
    "    \n",
    "for key in models_dict_cs.keys():\n",
    "    for subkey in models_dict_cs[key].keys():\n",
    "        if subkey != 'F':\n",
    "    \n",
    "            print(key, subkey, max(models_dict_cs[key][subkey]), sum(models_dict_cs[key][subkey])/len(models_dict_cs[key][subkey]))\n",
    "    for measure in measures:\n",
    "        \n",
    "        if max(models_dict_cs[key][measure]) > maxims_cs[measure][0]:\n",
    "            maxims_cs[measure][0] = round(max(models_dict_cs[key][measure]),6)\n",
    "            ##add models here\n",
    "            maxims_cs[measure][1] = [key]\n",
    "        elif max(models_dict_cs[key][measure]) == maxims_cs[measure][0]:\n",
    "            maxims_cs[measure][1].append(key)\n",
    "            \n",
    "    \n",
    "maxims_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Investigating maxim precision\n",
    "models_dict_cs['log_reg']['precision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_consider = 'precision'\n",
    "model = maxims_cs[to_consider][1][0]\n",
    "print(model)\n",
    "models_dict_cs[model][to_consider] = [round(x,6) for x in models_dict_cs[model][to_consider]]\n",
    "fea_index = models_dict_cs[model][to_consider].index(maxims_cs[to_consider][0])\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = preparing_X_and_y(df, features[fea_index], target)\n",
    "    \n",
    "clf = models_dict_cs[model]['F'].fit(X_train, y_train)\n",
    "    \n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"Recall: \", recall_score(y_test, pred),  \"ROC-AUC: \", roc_auc)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area ={})'.format(round(roc_auc, 4)))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "print(\"Flase Negatives: \", fn)\n",
    "print(\"Amount of ones predicted: \", pred.sum()/len(y_test))\n",
    "print(\"Features: \", features[fea_index])\n",
    "print(to_consider, savings_score(y_test,pred, cost_mat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_cs = pd.DataFrame(columns=measures.keys())\n",
    "\n",
    "\n",
    "for model in models_dict_cs.keys():\n",
    " \n",
    "    resultats_cs.loc[model] = [sum(models_dict_cs[model][measure])/len(models_dict_cs[model][measure])\\\n",
    "                               for measure in measures.keys()]\n",
    "    \n",
    "resultats_cs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Bayes Minimum Risk Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict_bmr = {'log_reg': {'F': LogisticRegression(C= 100, random_state=random_state)},\n",
    "'forest':{'F': RandomForestClassifier(class_weight='balanced', max_depth=5, n_estimators=10, max_features=1, random_state=random_state)}, \n",
    "'gbc': {'F': GradientBoostingClassifier(random_state=random_state, learning_rate=0.01)},\n",
    "       'svc':{'F': SVC(C=1.0, random_state=random_state, class_weight='balanced', probability=True)},\n",
    "              'MLPClassifier': {'F': MLPClassifier(random_state=random_state)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = {\"f1_score\": f1_score, \"precision\": precision_score, \n",
    "            \"recall\": recall_score, \"accuracy\": accuracy_score, 'roc_auc': roc_auc_score,\\\n",
    "           \"savings\": savings_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_metrics(mydict,features):\n",
    "\n",
    "    for key in mydict.keys():\n",
    "\n",
    "        print(key)\n",
    "        accuracies = []\n",
    "        recalls =[]\n",
    "        precisions = []\n",
    "        f1s = []\n",
    "        roc_aucs = []\n",
    "        savings = []\n",
    "\n",
    "\n",
    "        for i in features:\n",
    "\n",
    "            X_train, X_test, y_train, y_test = preparing_X_and_y(df, i, target)\n",
    "\n",
    "            clf = mydict[key]['F'].fit(X_train, y_train)\n",
    "\n",
    "            pred = clf.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, pred)\n",
    "            recall = recall_score(y_test,pred)\n",
    "            f1 = f1_score(y_test, pred)\n",
    "            precision = precision_score(y_test,pred)\n",
    "            accuracies.append(accuracy)\n",
    "            recalls.append(recall)\n",
    "            precisions.append(precision)\n",
    "            f1s.append(f1)\n",
    "            fpr, tpr, _ = roc_curve(y_test, pred)\n",
    "            roc_auc = roc_auc_score(y_test,pred)\n",
    "            roc_aucs.append(roc_auc)\n",
    "            ###define clas_mat_test\n",
    "\n",
    "            cost_mat_test = np.zeros((X_test.shape[0], 4))\n",
    "            for i in range(0,4):\n",
    "                cost_mat_test[:, i] = costs[i]   \n",
    "            saving = savings_score(y_test, pred, cost_mat_test)\n",
    "            savings.append(saving)\n",
    "\n",
    "\n",
    "        mydict[key]['accuracy'] = accuracies\n",
    "        mydict[key]['recall'] = recalls\n",
    "        mydict[key]['precision'] = precisions\n",
    "        mydict[key]['f1_score'] = f1s\n",
    "        mydict[key]['roc_auc'] = roc_aucs\n",
    "        mydict[key]['savings'] = savings\n",
    "\n",
    "    with open(mydict + 'mydict.pkl', 'wb') as picklefile:\n",
    "        pickle.dump(mydict, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmr_features = dict_features['recall_features_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models_dict_bmr = calculating_metrics(models_dict_bmr, bmr_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models_dict_bmr.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(models_dict_bmr, picklefile )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_dict_maxims(mydict,measures):\n",
    "\n",
    "    maxims = {}\n",
    "\n",
    "    for item in measures:\n",
    "        maxims[item] = [0, ['']]\n",
    "    \n",
    "    \n",
    "    for key in mydict.keys():\n",
    "    \n",
    "        print('Accuracy: ', key,max(mydict[key]['accuracy']))\n",
    "        print(\"Recall: \", key, max(mydict[key]['recall']))\n",
    "        print(\"Roc_auc: \", key, max(mydict[key]['roc_auc']))\n",
    "        for measure in measures:\n",
    "        \n",
    "            if max(mydict[key][measure]) > maxims[measure][0]:\n",
    "                maxims[measure][0] = round(max(mydict[key][measure]),6)\n",
    "                \n",
    "                maxims[measure][1] = [key]\n",
    "            elif max(mydict[key][measure]) == maxims[measure][0]:\n",
    "                maxims[measure][1].append(key)\n",
    "                \n",
    "    return maxims\n",
    "            \n",
    "    \n",
    "maxims_bmr = creating_dict_maxims(models_dict_bmr, measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Bayes Minimum Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preparing_X_and_y(df, features[41], target)\n",
    "\n",
    "for model in models_dict_bmr.keys():\n",
    "    print(model)\n",
    "    # Fit\n",
    "    models_dict_bmr[model][\"F\"].fit(X_train, y_train)\n",
    "    # Predict\n",
    "    models_dict_bmr[model][\"c\"] = models_dict_bmr[model][\"F\"].predict(X_test)\n",
    "    models_dict_bmr[model][\"p\"] = models_dict_bmr[model][\"F\"].predict_proba(X_test)\n",
    "    #models_dict_bmr[model][\"p_train\"] = models_dict_bmr[model][\"F\"].predict_proba(X_train)\n",
    "models_dict_bmr.keys()\n",
    "\n",
    "with open('models_dict_bmr.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(models_dict_bmr, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models_dict_bmr.pkl', 'rb') as picklefile:\n",
    "    models_dict_bmr = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtaining_resultats(mydict, measures):\n",
    "    \n",
    "    \n",
    "    resultats = pd.DataFrame(columns=measures.keys())\n",
    "    \n",
    "    for model in mydict.keys():\n",
    " \n",
    "        resultats.loc[model] = [measures[measure](y_test, mydict[model][\"c\"]) if measure != 'savings' else measures[measure](y_test, models_dict_bmr[model][\"c\"], cost_mat_test)\\\n",
    " for measure in measures.keys()]\n",
    "    \n",
    "    return resultats\n",
    "\n",
    "resultats_bmr = obtaining_resultats(models_dict_bmr, measures)\n",
    "\n",
    "resultats_bmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del models_dict_bmr['CSDT']\n",
    "models_dict_bmr.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for model in list(models_dict_bmr):\n",
    "    models_dict_bmr[model+\"-BMR\"] = {\"F\": BayesMinimumRiskClassifier()}\n",
    "    # Fit\n",
    "    models_dict_bmr[model+\"-BMR\"][\"F\"].fit(y_test, models_dict_bmr[model][\"p\"])  \n",
    "    # Calibration must be made in a validation set\n",
    "    # Predict\n",
    "    models_dict_bmr[model+\"-BMR\"][\"c\"] = models_dict_bmr[model+\"-BMR\"][\"F\"].predict(models_dict_bmr[model][\"p\"], cost_mat_test)\n",
    "    # Evaluate\n",
    "    resultats_bmr.loc[model+\"-BMR\"] = 0\n",
    "    resultats_bmr.loc[model+\"-BMR\", measures.keys()] = \\\n",
    "    [measures[measure](y_test, models_dict_bmr[model+\"-BMR\"][\"c\"]) if measure != 'savings' else measures[measure](y_test, models_dict_bmr[model][\"c\"], cost_mat_test)\\\n",
    " for measure in measures.keys()]    \n",
    "resultats_bmr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Adding costcla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costcla.models import CostSensitiveDecisionTreeClassifier, CostSensitiveLogisticRegression, CostSensitiveRandomForestClassifier\n",
    " \n",
    "    \n",
    "models_dict_bmr[\"CSDT\"] = {\"F\": CostSensitiveDecisionTreeClassifier(criterion_weight='balanced')}\n",
    "models_dict_bmr['CS-LR'] = {\"F\": CostSensitiveLogisticRegression(random_state=random_state)}\n",
    "models_dict_bmr['CSRC'] = {\"F\": CostSensitiveRandomForestClassifier(combination=\"weighted_voting\")}\n",
    "for i in ['CSDT', \"CS-LR\", 'CSRC']:\n",
    "    # Fit\n",
    "    models_dict_bmr[i][\"F\"].fit(X_train, y_train, cost_mat_train)\n",
    "    # Predict\n",
    "    models_dict_bmr[i][\"c\"] = models_dict_bmr[key][\"F\"].predict(X_test)\n",
    "    # Evaluate\n",
    "    resultats_bmr.loc[i] = 0\n",
    "    resultats_bmr.loc[i, measures.keys()] = \\\n",
    "    [measures[measure](y_test, models_dict_bmr[i][\"c\"]) if measure != 'savings' else measures[measure](y_test, models_dict_bmr[i][\"c\"], cost_mat_test)\\\n",
    "     for measure in measures.keys()]  \n",
    "    #[measures[measure](y_test, classifiers[\"CSRP\"][\"c\"]) for measure in measures.keys()]    \n",
    "print(resultats_bmr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ThresholdingOptimization\n",
    "\n",
    "# One soluntion via Bayes following guy in internet and one singular set of features\n",
    "\n",
    "# Finalize it with Fuzzy SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats[\"sav\"] = np.zeros(resultats.shape[0])\n",
    "for model in models_dict_cs.keys():\n",
    "    resultats[\"sav\"].loc[model] = (savings_score(y_test, models_dict_cs[model][\"class\"], cost_mat_test))\n",
    "resultats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\"RF\": {\"f\": RandomForestClassifier(random_state=random_state)},\n",
    "               \"DT\": {\"f\": DecisionTreeClassifier(random_state=random_state)},\n",
    "               \"LR\": {\"f\": LogisticRegression(random_state=random_state)}}\n",
    "\n",
    "for model in classifiers.keys():\n",
    "    # Fit\n",
    "    classifiers[model][\"f\"].fit(X_train, y_train)\n",
    "    # Predict\n",
    "    classifiers[model][\"c\"] = classifiers[model][\"f\"].predict(X_test)\n",
    "    classifiers[model][\"p\"] = classifiers[model][\"f\"].predict_proba(X_test)\n",
    "    classifiers[model][\"p_train\"] = classifiers[model][\"f\"].predict_proba(X_train)\n",
    "classifiers.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = {\"f1\": f1_score, \"pre\": precision_score, \n",
    "            \"rec\": recall_score, \"acc\": accuracy_score, 'roc_auc': roc_auc_score}\n",
    "results = pd.DataFrame(columns=measures.keys())\n",
    "\n",
    "# Evaluate each model in classifiers\n",
    "for model in classifiers.keys():\n",
    " \n",
    "    results.loc[model] = [measures[measure](y_test, classifiers[model][\"c\"]) for measure in measures.keys()]\n",
    "    \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costcla.metrics import savings_score, cost_loss \n",
    "\n",
    "# Evaluate the savings for each model\n",
    "results[\"sav\"] = np.zeros(results.shape[0])\n",
    "for model in classifiers.keys():\n",
    "    results[\"sav\"].loc[model] = (savings_score(y_test, classifiers[model][\"c\"], cost_mat_test))\n",
    "results[\"cost_vs_baseline\"] = np.zeros(results.shape[0])\n",
    "for model in classifiers.keys():\n",
    "    results[\"cost_vs_baseline\"].loc[model] = (cost_loss(y_test, classifiers[model][\"c\"], cost_mat_test) - cost_all_zeros)\n",
    "\n",
    "# TODO: plot results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the results\n",
    "colors = sns.color_palette()\n",
    "\n",
    "figsize = (10, 5)\n",
    "ax = plt.subplot(111)\n",
    "l = ax.plot(ind, results[\"f1\"], \"-o\", label='F1Score', color=colors[2])\n",
    "b = ax.bar(ind-0.3, results['sav'], 0.6, label='Savings', color=colors[0])\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "ax.set_xlim([-0.5, ind[-1]+.5])\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(results.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costcla.models import BayesMinimumRiskClassifier\n",
    "ci_models = models_dict_cs.keys()\n",
    "\n",
    "for model in list(models_dict_cs):\n",
    "    classifiers[model+\"-BMR\"] = {\"F\": BayesMinimumRiskClassifier()}\n",
    "    # Fit\n",
    "    classifiers[model+\"-BMR\"][\"F\"].fit(y_test, classifiers[model][\"p\"])  \n",
    "    # Calibration must be made in a validation set\n",
    "    # Predict\n",
    "    classifiers[model+\"-BMR\"][\"class\"] = classifiers[model+\"-BMR\"][\"F\"].predict(classifiers[model][\"p\"], cost_mat_test)\n",
    "    # Evaluate\n",
    "    results.loc[model+\"-BMR\"] = 0\n",
    "    results.loc[model+\"-BMR\", measures.keys()] = \\\n",
    "    [measures[measure](y_test, classifiers[model+\"-BMR\"][\"c\"]) for measure in measures.keys()]\n",
    "    results[\"sav\"].loc[model+\"-BMR\"] = savings_score(y_test, classifiers[model+\"-BMR\"][\"c\"], cost_mat_test)\n",
    "    \n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "ind = np.arange(results.shape[0])\n",
    "colors = sns.color_palette()\n",
    "figsize = (10, 5)\n",
    "ax = plt.subplot(111)\n",
    "l = ax.plot(ind, results[\"f1\"], \"-o\", label='F1Score', color=colors[2])\n",
    "b = ax.bar(ind-0.3, results['sav'], 0.6, label='Savings', color=colors[0])\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "ax.set_xlim([-0.5, ind[-1]+.5])\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(results.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costcla.models import CostSensitiveRandomPatchesClassifier\n",
    "\n",
    "classifiers[\"CSRP\"] = {\"f\": CostSensitiveRandomPatchesClassifier(combination='weighted_voting')}\n",
    "# Fit\n",
    "classifiers[\"CSRP\"][\"f\"].fit(X_train, y_train, cost_mat_train)\n",
    "# Predict\n",
    "classifiers[\"CSRP\"][\"c\"] = classifiers[\"CSRP\"][\"f\"].predict(X_test)\n",
    "# Evaluate\n",
    "results.loc[\"CSRP\"] = 0\n",
    "results.loc[\"CSRP\", measures.keys()] = \\\n",
    "[measures[measure](y_test, classifiers[\"CSRP\"][\"c\"]) for measure in measures.keys()]\n",
    "results[\"sav\"].loc[\"CSRP\"] = savings_score(y_test, classifiers[\"CSRP\"][\"c\"], cost_mat_test)\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "ind = np.arange(results.shape[0])\n",
    "figsize = (10, 5)\n",
    "ax = plt.subplot(111)\n",
    "l = ax.plot(ind, results[\"f1\"], \"-o\", label='F1Score', color=colors[2])\n",
    "b = ax.bar(ind-0.3, results['sav'], 0.6, label='Savings', color=colors[0])\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "ax.set_xlim([-0.5, ind[-1]+.5])\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(results.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "QuadraticDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "from operator import itemgetter\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "# Utility function to report best scores\n",
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "# from http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html\n",
    "param_dist = {\"n_estimators\": [10, 20, 50, 100, 1000],\n",
    "              \"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 10),\n",
    "              \"min_samples_split\": sp_randint(1, 100),\n",
    "              \"min_samples_leaf\": sp_randint(1, 100),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run randomized search\n",
    "classifiers[\"RS-RF\"] = {\"f\": RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                                n_iter=20, n_jobs=4, verbose=1)}\n",
    "# Fit\n",
    "start = time()\n",
    "classifiers[\"RS-RF\"][\"f\"].fit(X_train, y_train)\n",
    "print(\"RandomizedSearchCV took %.2f seconds\"\n",
    "      \" parameter settings.\" % ((time() - start),))\n",
    "\n",
    "report(classifiers[\"RS-RF\"][\"f\"].grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "classifiers[\"RS-RF\"][\"c\"] = classifiers[\"RS-RF\"][\"f\"].predict(X_test)\n",
    "# Evaluate\n",
    "results.loc[\"RS-RF\"] = 0\n",
    "results.loc[\"RS-RF\", measures.keys()] = \\\n",
    "[measures[measure](y_test, classifiers[\"RS-RF\"][\"c\"]) for measure in measures.keys()]\n",
    "results[\"sav\"].loc[\"RS-RF\"] = savings_score(y_test, classifiers[\"RS-RF\"][\"c\"], cost_mat_test)\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_acc = []\n",
    "rf_rec = []\n",
    "rf_auc = []\n",
    "rf_fn = []\n",
    "\n",
    "for item in features:\n",
    "    \n",
    "    \n",
    "    y = train_upsampled.victims_alert\n",
    "    X = preprocessing_features(train_upsampled, item)\n",
    "    y_test = test.victims_alert\n",
    "    X_test = preprocessing_features(test, item) \n",
    "\n",
    "\n",
    "    clf = RandomForestClassifier(random_state=random_state).fit(X,y)\n",
    "\n",
    "    # Predict on training set\n",
    "    pred = clf.predict(X_test)\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    _, _, fn, _ = confusion_matrix(y_test, pred).ravel()\n",
    "    \n",
    "    rf_fn.append(fn)\n",
    "    print( 'Random Forest False Negative: ', fn )\n",
    "    # How's our accuracy?\n",
    "    \n",
    "    rf_acc.append(accuracy_score(y_test, pred))\n",
    "    \n",
    "    #print( 'Random Forest Accuracy: ', round(accuracy_score(y_test, pred),4) )\n",
    "    rf_rec.append(recall_score(y_test, pred))\n",
    "\n",
    "    #print('Random Forest Recall: ',round(recall_score(y_test, pred), 4))\n",
    "    rf_auc.append(auc(fpr, tpr))\n",
    "    #print('Random Forest AUC: ',round(auc(fpr, tpr), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(rf_fn), rf_fn.index(min(rf_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(rf_acc), rf_acc.index(max(rf_acc)), max(rf_rec), rf_rec.index(max(rf_rec)), max(rf_auc), rf_auc.index(max(rf_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_upsampled.victims_alert\n",
    "X = preprocessing_features(train_upsampled, features[35])\n",
    "y_test = test.victims_alert\n",
    "X_test = preprocessing_features(test, features[35]) \n",
    "\n",
    "# Train model\n",
    "\n",
    "clf = RandomForestClassifier(random_state=random_state).fit(X,y)\n",
    "\n",
    "# Predict on training set\n",
    "\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "prob = clf.predict_proba(X_test)\n",
    "\n",
    " \n",
    "# Is our model still predicting just one class?\n",
    "#print( np.unique( pred_y_1 ) )\n",
    "# [0 1]\n",
    " \n",
    "# How's our accuracy?\n",
    "\n",
    "print( 'Random Forest Accuracy: ', round(accuracy_score(y_test, pred),4) )\n",
    "\n",
    "print('Random Forest Recall: ',round(recall_score(y_test, pred), 4))\n",
    "\n",
    "\n",
    "\n",
    "print( 'Random Forest: ',pred.sum(), len(pred), len(pred) *0.25, pred.sum()/len(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_0 = [x[0] for x in prob]\n",
    "prob_1 = [x[1] for x in prob]\n",
    "\n",
    "\n",
    "for item in np.linspace(0.5,1, 5):\n",
    "    \n",
    "    pred_0_t = [0 if x >= item else 1 for x in prob_0 ]\n",
    "    acc_0_t = accuracy_score(y_test, pred_0_t)\n",
    "    rec_0_t = recall_score(y_test,pred_0_t)\n",
    "    prec_0_t = precision_score(y_test, pred_0_t)\n",
    "    _, _, fn_0_t, _ = confusion_matrix(y_test, pred_0_t).ravel()\n",
    "    print(item, acc_0_t, rec_0_t, fn_0_t, prec_0_t, sum(pred_0_t), len(pred_0_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_features, rf_rec);\n",
    "for key in dict_features.keys():\n",
    "    plt.axvline(x=dict_features[key], c = 'red')\n",
    "plt.title(\"Recall(acc, f1, precision, recall)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_features = [x for x in range(1,len(features) + 1)]\n",
    "plt.plot(x_features, rf_acc);\n",
    "for key in dict_features.keys():\n",
    "    plt.axvline(x=dict_features[key], c = 'red')\n",
    "plt.title(\"Accuracy(acc, f1, precision, recall)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for key in dict_features.keys():\n",
    "    plt.axvline(x=dict_features[key], c = 'red')\n",
    "plt.plot(x_features, rf_auc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score \n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "RNG = 42\n",
    "# Make some synthetic imbalanced binary classification data\n",
    "y = train_upsampled.victims_alert\n",
    "X = preprocessing_features(train_upsampled, features[39])\n",
    "y_test = test.victims_alert\n",
    "X_test = preprocessing_features(test, features[39]) \n",
    "\n",
    "metric_names = ['f1', 'average_precision', 'accuracy', 'precision', 'recall']\n",
    "scores_df = pd.DataFrame(index=metric_names, columns=['Random-CV', 'Stratified-CV']) # to store the scores\n",
    "cv = KFold(n_splits=3)\n",
    "scv = StratifiedKFold(n_splits=3)\n",
    "clf = RandomForestClassifier(random_state=random_state)\n",
    "for metric in metric_names:\n",
    "    score1 = cross_val_score(clf, X, y, scoring=metric, cv=cv).mean()\n",
    "    score2 = cross_val_score(clf, X, y, scoring=metric, cv=scv).mean()\n",
    "    scores_df.loc[metric] = [score1, score2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in np.linspace(0.3,0.5, 4):\n",
    "    for model in models_dict.keys():\n",
    "    \n",
    "        print(r, models_dict[model]['name'], selecting_model_smote(df, target, r, features[39], models_dict[model]['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_smote_perceptron = (0.2/3 + 0.3)\n",
    "a, b, _, _, pred, y_test  = selecting_model_smote\\\n",
    "(df,target,max_smote_perceptron,features[39],models_dict['Perceptron']['name'])\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Random Forest confusion matrix analysis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "\n",
    "from sklearn.metrics import auc, precision_recall_curve, average_precision_score\n",
    "fpr, tpr, _ = roc_curve(y_test, pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area ={})'.format(round(roc_auc, 4)))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn, sum(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Trying SVC with penalisation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def selecting_model_smote(dataframe, target, ratio, features,model):\n",
    "    \n",
    "    data = dataframe[features].copy()\n",
    "    data.reset_index(inplace=True)\n",
    "    for item in features:\n",
    "        if data[item].dtypes == 'object':\n",
    "            #print(item, data[item].dtypes, 'object')\n",
    "            prova = pd.get_dummies(data[item], drop_first=True)\n",
    "            data = pd.concat([data,prova], axis=1)\n",
    "            data.drop(item, axis=1, inplace=True)\n",
    "\n",
    "    X = np.array(data)\n",
    "    y = dataframe[target]\n",
    "    y = (np.where (y > 0, 1, 0))\n",
    "    y = np.reshape(y, len(y))\n",
    "    \n",
    "    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=1, stratify=y)\n",
    "    \n",
    "    \n",
    "    X_resampled, y_resampled = SMOTE(ratio=ratio, random_state=2018).fit_sample(X_train, y_train)\n",
    "\n",
    "    #print('Labels counts in y:', np.bincount(y), (np.bincount(y)[1]/np.bincount(y)[0]))\n",
    "    #print('Labels counts in y_train:', np.bincount(y_train), (np.bincount(y_train)[1]/np.bincount(y_train)[0]))\n",
    "    #print('Labels counts in y_test:', np.bincount(y_test), (np.bincount(y_test)[1]/np.bincount(y_test)[0]))\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(X_resampled)\n",
    "    X_train_std = ss.transform(X_resampled)\n",
    "    X_test_std = ss.transform(X_test)\n",
    "    \n",
    "\n",
    "                \n",
    "    model.fit(X_train_std, y_resampled)\n",
    "    pred = model.predict(X_test_std)\n",
    "    #prob = clf.predict_proba(X_test_std)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    f1 = f1_score(y_test, pred)\n",
    "        \n",
    "    return accuracy, recall, precision, f1, pred, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###chapter 2\n",
    "\n",
    "class Perceptron(object):\n",
    "    \"\"\"Perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "      Passes over the training dataset.\n",
    "    random_state : int\n",
    "      Random number generator seed for random weight\n",
    "      initialization.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 1d-array\n",
    "      Weights after fitting.\n",
    "    errors_ : list\n",
    "      Number of misclassifications (updates) in each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_samples, n_features]\n",
    "          Training vectors, where n_samples is the number of samples and\n",
    "          n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples]\n",
    "          Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
    "        self.errors_ = []\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                update = self.eta * (target - self.predict(xi))\n",
    "                self.w_[1:] += update * xi\n",
    "                self.w_[0] += update\n",
    "                errors += int(update != 0.0)\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def selecting_model(dataframe, target, features, **models_dict):\n",
    "    \n",
    "    data = dataframe[features].copy()\n",
    "    data.reset_index(inplace=True)\n",
    "    for item in features:\n",
    "        if data[item].dtypes == 'object':\n",
    "            #print(item, data[item].dtypes, 'object')\n",
    "            prova = pd.get_dummies(data[item], drop_first=True)\n",
    "            data = pd.concat([data,prova], axis=1)\n",
    "            data.drop(item, axis=1, inplace=True)\n",
    "\n",
    "    X = np.array(data)\n",
    "    y = dataframe[target]\n",
    "    y = (np.where (y > 0, 1, 0))\n",
    "    y = np.reshape(y, len(y))\n",
    "    \n",
    "    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=1, stratify=y)\n",
    "    \n",
    "    \n",
    "    X_resampled, y_resampled = SMOTE(ratio=0.3, random_state=2018).fit_sample(X_train, y_train)\n",
    "\n",
    "    #print('Labels counts in y:', np.bincount(y), (np.bincount(y)[1]/np.bincount(y)[0]))\n",
    "    #print('Labels counts in y_train:', np.bincount(y_train), (np.bincount(y_train)[1]/np.bincount(y_train)[0]))\n",
    "    #print('Labels counts in y_test:', np.bincount(y_test), (np.bincount(y_test)[1]/np.bincount(y_test)[0]))\n",
    "\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(X_resampled)\n",
    "    X_train_std = ss.transform(X_resampled)\n",
    "    X_test_std = ss.transform(X_test)\n",
    "    \n",
    "    \n",
    "    for item in models_dict.keys():\n",
    "\n",
    "        \n",
    "  \n",
    "        clf = models_dict[item]['name']\n",
    "            \n",
    "        clf.fit(X_train_std, y_resampled)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        pred = clf.predict(X_test_std)\n",
    "        #prob = clf.predict_proba(X_test_std)\n",
    "        accuracy = accuracy_score(y_test, pred)\n",
    "        recall = recall_score(y_test, pred)\n",
    "        #models_dict[item]['pred'].append(pred)\n",
    "        #models_dict[item]['proba'].append(prob)\n",
    "        #models_dict[item]['accuracy'].append(accuracy)\n",
    "        #models_dict[item]['recall'].append(recall)\n",
    "        #models_dict[key]['count'].append(num)\n",
    "        print(str(models_dict[item]))\n",
    "        print('Misclassified samples: %d' % (y_test != pred).sum())\n",
    "        print('Amount of victims predicted: ', pred.sum())\n",
    "        print(\"How many predictions over real: \", pred.sum()/y_test.sum())\n",
    "        print(\"Predicted one vs total compared with original: \", pred.sum()/len(y_test), y_test.sum()/len(y_test))\n",
    "        print('Victims_alert missed: ',  sum([1 for x,y in zip(y_test,pred) if x ==1 and y==0]))\n",
    "\n",
    "        print('Accuracy: %.4f' % accuracy_score(y_test, pred))\n",
    "        print('Recall: {}'.format(recall_score(y_test, pred)))\n",
    "        print('Precision: {}'.format(precision_score(y_test, pred)))\n",
    "        print('f1_score: {}'.format(f1_score(y_test, pred)))\n",
    "        print('fbeta_score: {}'.format(fbeta_score(y_test, pred, beta=0.5)))\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selecting_model(df, target,features_max_f1_score[1], **models_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_features(df, features):\n",
    "    \n",
    "    \"Getting ready the features for the model\"\n",
    "    \n",
    "    data = df[features].copy()\n",
    "    data.reset_index(inplace=True)\n",
    "    for item in features:\n",
    "        if data[item].dtypes == 'object':\n",
    "            print(item, data[item].dtypes)\n",
    "            prova = pd.get_dummies(data[item], drop_first=True)\n",
    "            data = pd.concat([data,prova], axis=1)\n",
    "            data.drop(item, axis=1, inplace=True)\n",
    "\n",
    "    features_array = np.array(data)\n",
    "    \n",
    "    return features_array\n",
    "preprocessing_features(df,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,0,1,0,0,0])\n",
    "b = np.array([0,0,1,0,0,0])\n",
    "sum([1 for x,y in zip(a,b) if x ==1 and y==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_array = preprocessing_features(df, all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features_array\n",
    "y = df[target].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class SBS():\n",
    "    \n",
    "    \"\"\"loading initial information\"\"\"\n",
    "    \n",
    "    def __init__(self, estimator, k_features, scoring=accuracy_score,\n",
    "                 test_size=0.25, random_state=1):\n",
    "        self.scoring = scoring\n",
    "        self.estimator = clone(estimator)\n",
    "        self.k_features = k_features\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        \"\"\"calculate score with all features included initialy in X\"\"\"\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            train_test_split(X, y, test_size=self.test_size,\n",
    "                             random_state=self.random_state)\n",
    "\n",
    "        dim = X_train.shape[1]\n",
    "        self.indices_ = tuple(range(dim))\n",
    "        self.subsets_ = [self.indices_]\n",
    "        score = self._calc_score(X_train, y_train, \n",
    "                                 X_test, y_test, self.indices_)\n",
    "        self.scores_ = [score]\n",
    "        \n",
    "        \"while the number of initial features is bigger than the value k_features given\"\n",
    "\n",
    "        while dim > self.k_features:\n",
    "            scores = []\n",
    "            subsets = []\n",
    "\n",
    "            for p in combinations(self.indices_, r=dim - 1):\n",
    "                score = self._calc_score(X_train, y_train, \n",
    "                                         X_test, y_test, p)\n",
    "                scores.append(score)\n",
    "                subsets.append(p)\n",
    "\n",
    "            best = np.argmax(scores)\n",
    "            self.indices_ = subsets[best]\n",
    "            self.subsets_.append(self.indices_)\n",
    "            dim -= 1\n",
    "\n",
    "            self.scores_.append(scores[best])\n",
    "        self.k_score_ = self.scores_[-1]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \n",
    "        \"\"\"Preparing the X based on selected features\"\"\"\n",
    "        return X[:, self.indices_]\n",
    "\n",
    "    def _calc_score(self, X_train, y_train, X_test, y_test, indices):\n",
    "        \n",
    "        \"\"\"scoreing the model based on gthe selected features selected by indices\"\"\"\n",
    "        \n",
    "        self.estimator.fit(X_train[:, indices], y_train)\n",
    "        y_pred = self.estimator.predict(X_test[:, indices])\n",
    "        score = self.scoring(y_test, y_pred)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_fetuares = df[all_features].select_dtypes(exclude='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df[numeric_fetuares])\n",
    "y = df[target].values\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "# selecting features\n",
    "sbs = SBS(knn, k_features=1)\n",
    "#sbs.fit(X_train_std, y_train)\n",
    "sbs.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _calc_score(X_train, y_train, X_test, y_test):\n",
    "        pp = Perceptron()\n",
    "        pp.fit(X_train, y_train)\n",
    "        y_pred = pp.predict(X_test)\n",
    "        score = accuracy_score(y_test, y_pred)\n",
    "        return score, y_pred, y_test\n",
    "    \n",
    "dim = X_train.shape[1]\n",
    "indices_ = tuple(range(dim))\n",
    "subsets_ = [indices_]\n",
    "score, pred, real = _calc_score(X_train, y_train, \n",
    "                                 X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred), len(real), pred.sum(), real.sum()\n",
    "misclassified_samples = []\n",
    "cagades = []\n",
    "\n",
    "for x,y in zip(real,pred):\n",
    "    if x != y:\n",
    "        misclassified_samples.append(1)\n",
    "    \n",
    "print('Misclassified Samples: ', sum(misclassified_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['Perceptron', 'LogisticRegression', 'SVC', 'DecisionTreeClassifier', 'RandomForestClassifier',\\\n",
    "          'KNeighborsClassifier']\n",
    "for itrem in models:    \n",
    "    calculant_recall(df,target, features, itrem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    print(i, len(df[i].unique()))\n",
    "\n",
    "print('Number of Nan;', df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred), y_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculate recall\n",
    "evaluation_metrics['recall'], evaluation_metrics['precision']\n",
    "##THIS ONE SUCKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SECOND TRY-----ADDING FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##we will add some features\n",
    "set_features = ['hour_day', 'month', 'latitude', 'longitude', 'year', 'victims_alert']\n",
    "\n",
    "df1 = (pd.concat([df[set_features], pd.get_dummies(df['month'])], axis=1, join_axes=[df.index])).drop('month', axis= 1)  \n",
    "\n",
    "first_set_features = df1.columns\n",
    "first_set_features.drop('victims_alert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(df1):\n",
    "    \"\"\"Prepares input features from accidents.\n",
    "\n",
    "    Args:\n",
    "    final_accidents dataframe: A Pandas DataFrame expected to contain data\n",
    "      from the Barcelona Accidents.\n",
    "    Returns:\n",
    "    A DataFrame that contains the features to be used for the model, including\n",
    "    synthetic features.\n",
    "    \"\"\"\n",
    "    selected_features = df1[first_set_features]\n",
    "    processed_features = selected_features.copy()\n",
    "    return processed_features\n",
    "\n",
    "def preprocess_targets(df1):\n",
    "    \"\"\"Prepares target features (i.e., labels) from BarcelonaAccidents data set.\n",
    "\n",
    "    Args:\n",
    "    final_accidents_dataframe: A Pandas DataFrame expected to contain data\n",
    "      from the BarcelonaAccidents data set.\n",
    "    Returns:\n",
    "    A DataFrame that contains the target feature.\n",
    "    \"\"\"\n",
    "    output_targets = pd.DataFrame()\n",
    "    # Create a boolean categorical feature representing whether the\n",
    "    # median_house_value is above a set threshold.\n",
    "    output_targets[\"victims_alert\"] = df1[\"victims_alert\"].astype(float)\n",
    "    return output_targets\n",
    "\n",
    "msk = np.random.rand(len(df1)) < 0.75\n",
    "# Choose the first 12000 (out of 17000) examples for training.\n",
    "training_examples = preprocess_features(df1[msk])\n",
    "training_targets = preprocess_targets(df1[msk])\n",
    "\n",
    "# Choose the last 5000 (out of 17000) examples for validation.\n",
    "validation_examples = preprocess_features(df1[~msk])\n",
    "validation_targets = preprocess_targets(df1[~msk])\n",
    "\n",
    "# Double-check that we've done the right thing.\n",
    "print(\"Training examples summary:\")\n",
    "display.display(training_examples.describe())\n",
    "print(\"Validation examples summary:\")\n",
    "display.display(validation_examples.describe())\n",
    "\n",
    "print(\"Training targets summary:\")\n",
    "display.display(training_targets.describe())\n",
    "print(\"Validation targets summary:\")\n",
    "display.display(validation_targets.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_linear_classifier_model(\n",
    "    learning_rate,\n",
    "    regularization_strength,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    feature_columns,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "    \"\"\"Trains a linear regression model.\n",
    "\n",
    "    In addition to training, this function also prints training progress information,\n",
    "    as well as a plot of the training and validation loss over time.\n",
    "\n",
    "    Args:\n",
    "    learning_rate: A `float`, the learning rate.\n",
    "    regularization_strength: A `float` that indicates the strength of the L1\n",
    "       regularization. A value of `0.0` means no regularization.\n",
    "    steps: A non-zero `int`, the total number of training steps. A training step\n",
    "      consists of a forward and backward pass using a single batch.\n",
    "    feature_columns: A `set` specifying the input feature columns to use.\n",
    "    training_examples: A `DataFrame` containing one or more columns from\n",
    "      `california_housing_dataframe` to use as input features for training.\n",
    "    training_targets: A `DataFrame` containing exactly one column from\n",
    "      `california_housing_dataframe` to use as target for training.\n",
    "    validation_examples: A `DataFrame` containing one or more columns from\n",
    "      `california_housing_dataframe` to use as input features for validation.\n",
    "    validation_targets: A `DataFrame` containing exactly one column from\n",
    "      `california_housing_dataframe` to use as target for validation.\n",
    "\n",
    "    Returns:\n",
    "    A `LinearClassifier` object trained on the training data.\n",
    "    \"\"\"\n",
    "\n",
    "    periods = 12\n",
    "    steps_per_period = steps / periods\n",
    "\n",
    "    # Create a linear classifier object.\n",
    "    my_optimizer = tf.train.FtrlOptimizer(learning_rate=learning_rate, l1_regularization_strength=regularization_strength)\n",
    "    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "    linear_classifier = tf.estimator.LinearClassifier(\n",
    "      feature_columns=feature_columns,\n",
    "      optimizer=my_optimizer\n",
    "    )\n",
    "\n",
    "    # Create input functions.\n",
    "    training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                                          training_targets[\"victims_alert\"], \n",
    "                                          batch_size=batch_size)\n",
    "    predict_training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                                                  training_targets[\"victims_alert\"], \n",
    "                                                  num_epochs=1, \n",
    "                                                  shuffle=False)\n",
    "    predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n",
    "                                                    validation_targets[\"victims_alert\"], \n",
    "                                                    num_epochs=1, \n",
    "                                                    shuffle=False)\n",
    "\n",
    "    # Train the model, but do so inside a loop so that we can periodically assess\n",
    "    # loss metrics.\n",
    "    print(\"Training model...\")\n",
    "    print(\"LogLoss (on validation data):\")\n",
    "    training_log_losses = []\n",
    "    validation_log_losses = []\n",
    "    for period in range (0, periods):\n",
    "        # Train the model, starting from the prior state.\n",
    "        linear_classifier.train(\n",
    "            input_fn=training_input_fn,\n",
    "            steps=steps_per_period\n",
    "        )\n",
    "        # Take a break and compute predictions.\n",
    "        training_probabilities = linear_classifier.predict(input_fn=predict_training_input_fn)\n",
    "        training_probabilities = np.array([item['probabilities'] for item in training_probabilities])\n",
    "\n",
    "        validation_probabilities = linear_classifier.predict(input_fn=predict_validation_input_fn)\n",
    "        validation_probabilities = np.array([item['probabilities'] for item in validation_probabilities])\n",
    "\n",
    "        # Compute training and validation loss.\n",
    "        training_log_loss = metrics.log_loss(training_targets, training_probabilities)\n",
    "        validation_log_loss = metrics.log_loss(validation_targets, validation_probabilities)\n",
    "        # Occasionally print the current loss.\n",
    "        print(\"  period {:02d} : {:0.2f}\".format(period, validation_log_loss))\n",
    "        # Add the loss metrics from this period to our list.\n",
    "        training_log_losses.append(training_log_loss)\n",
    "        validation_log_losses.append(validation_log_loss)\n",
    "    print(\"Model training finished.\")\n",
    "    \n",
    "    \n",
    "\n",
    "    #validation_predictions = np.array([item['probabilities'] for item in validation_probabilities])\n",
    "\n",
    "    _ = plt.hist(validation_predictions)\n",
    "\n",
    "    evaluation_metrics = linear_classifier.evaluate(input_fn=predict_validation_input_fn)\n",
    "\n",
    "    print(\"AUC on the validation set: {:0.4f}\".format(evaluation_metrics['auc']))\n",
    "    print(\"Accuracy on the validation set: {:0.6f}\".format(evaluation_metrics['accuracy']))\n",
    "    print(\"AUC_precision-recall {:0.5f}\".format(evaluation_metrics['auc_precision_recall']))\n",
    "\n",
    "    # Output a graph of loss metrics over periods.\n",
    "    plt.ylabel(\"LogLoss\")\n",
    "    plt.xlabel(\"Periods\")\n",
    "    plt.title(\"LogLoss vs. Periods\")\n",
    "    plt.tight_layout()\n",
    "    plt.plot(training_log_losses, label=\"training\")\n",
    "    plt.plot(validation_log_losses, label=\"validation\")\n",
    "    plt.legend()\n",
    "\n",
    "    return linear_classifier\n",
    "\n",
    "def model_size(estimator):\n",
    "    variables = estimator.get_variable_names()\n",
    "    size = 0\n",
    "    for variable in variables:\n",
    "        if not any(x in variable \n",
    "            for x in ['global_step',\n",
    "                         'centered_bias_weight',\n",
    "                         'bias_weight',\n",
    "                         'Ftrl']\n",
    "              ):\n",
    "            size += np.count_nonzero(estimator.get_variable_value(variable))\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in np.linspace(0,1, 5):\n",
    "    linear_classifier = train_linear_classifier_model(\n",
    "        learning_rate=0.000005,\n",
    "        # TWEAK THE REGULARIZATION VALUE BELOW\n",
    "        regularization_strength =item,\n",
    "        steps=500,\n",
    "        batch_size=100,\n",
    "        feature_columns =construct_feature_columns(training_examples),\n",
    "        training_examples=training_examples,\n",
    "        training_targets=training_targets,\n",
    "        validation_examples=validation_examples,\n",
    "        validation_targets=validation_targets)\n",
    "    print(\"Model size:\", model_size(linear_classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n",
    "                                                  validation_targets[\"victims_alert\"], \n",
    "                                                  num_epochs=1, \n",
    "                                                  shuffle=False)\n",
    "\n",
    "validation_predictions = linear_classifier.predict(input_fn=predict_validation_input_fn)\n",
    "validation_predictions = np.array([item['probabilities'][0] for item in validation_predictions])\n",
    "\n",
    "_ = plt.hist(validation_predictions)\n",
    "\n",
    "evaluation_metrics = linear_classifier.evaluate(input_fn=predict_validation_input_fn)\n",
    "\n",
    "print(\"AUC on the validation set: {:0.4f}\".format(evaluation_metrics['auc']))\n",
    "print(\"Accuracy on the validation set: {:0.2f}\".format(evaluation_metrics['accuracy']))\n",
    "print(\"AUC_precision-recall {:0.3f}\".format(evaluation_metrics['auc_precision_recall']))\n",
    "\n",
    "validation_probabilities = linear_classifier.predict(input_fn=predict_validation_input_fn)\n",
    "# Get just the probabilities for the positive class.\n",
    "validation_probabilities = np.array([item['probabilities'][1] for item in validation_probabilities])\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(\n",
    "    validation_targets, validation_probabilities)\n",
    "plt.plot(false_positive_rate, true_positive_rate, label=\"our model\")\n",
    "plt.plot([0, 1], [0, 1], label=\"random classifier\")\n",
    "_ = plt.legend(loc=2)\n",
    "\n",
    "average_precision = average_precision_score(validation_targets, validation_probabilities)\n",
    "precision, recall, _ = precision_recall_curve(validation_targets, validation_probabilities)\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculate recall\n",
    "evaluation_metrics['recall'], evaluation_metrics['precision']\n",
    "##THIS ONE SUCKS but it doesn't?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THIRD TRY--- ALL FEATURES AND REGLARIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##we will add some features\n",
    "set_features = ['hour_day', 'month', 'latitude', 'longitude', 'year', 'victims_alert', 'week_day']\n",
    "\n",
    "df2 = (pd.concat([df[set_features], pd.get_dummies(df[['month', 'week_day']])], axis=1, join_axes=[df.index])).drop(['month', 'week_day'], axis= 1)  \n",
    "\n",
    "first_set_features = df2.columns\n",
    "\n",
    "def preprocess_features(df2):\n",
    "    \"\"\"Prepares input features from California housing data set.\n",
    "\n",
    "    Args:\n",
    "    california_housing_dataframe: A Pandas DataFrame expected to contain data\n",
    "      from the California housing data set.\n",
    "    Returns:\n",
    "    A DataFrame that contains the features to be used for the model, including\n",
    "    synthetic features.\n",
    "    \"\"\"\n",
    "    selected_features = df2[first_set_features]\n",
    "    processed_features = selected_features.copy()\n",
    "    processed_features[\"latxlong\"] = (\n",
    "    df2[\"latitude\"] * df2[\"longitude\"])\n",
    "    return processed_features\n",
    "\n",
    "def preprocess_targets(df2):\n",
    "    \"\"\"Prepares target features (i.e., labels) from California housing data set.\n",
    "\n",
    "    Args:\n",
    "    california_housing_dataframe: A Pandas DataFrame expected to contain data\n",
    "      from the California housing data set.\n",
    "    Returns:\n",
    "    A DataFrame that contains the target feature.\n",
    "    \"\"\"\n",
    "    output_targets = pd.DataFrame()\n",
    "    # Create a boolean categorical feature representing whether the\n",
    "    # median_house_value is above a set threshold.\n",
    "    output_targets[\"victims_alert\"] = df2[\"victims_alert\"].astype(float)\n",
    "    return output_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(df2)) < 0.75\n",
    "# Choose the first 12000 (out of 17000) examples for training.\n",
    "training_examples = preprocess_features(df2[msk])\n",
    "training_targets = preprocess_targets(df2[msk])\n",
    "\n",
    "# Choose the last 5000 (out of 17000) examples for validation.\n",
    "validation_examples = preprocess_features(df2[~msk])\n",
    "validation_targets = preprocess_targets(df2[~msk])\n",
    "\n",
    "# Double-check that we've done the right thing.\n",
    "print(\"Training examples summary:\")\n",
    "display.display(training_examples.describe())\n",
    "print(\"Validation examples summary:\")\n",
    "display.display(validation_examples.describe())\n",
    "\n",
    "print(\"Training targets summary:\")\n",
    "display.display(training_targets.describe())\n",
    "print(\"Validation targets summary:\")\n",
    "display.display(validation_targets.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##It is better to use a low learning rate to ensure convergence\n",
    "\n",
    "linear_classifier = train_linear_classifier_model(\n",
    "    learning_rate=0.000001,\n",
    "    # TWEAK THE REGULARIZATION VALUE BELOW\n",
    "    steps=500,\n",
    "    batch_size=20,\n",
    "    training_examples=training_examples,\n",
    "    training_targets=training_targets,\n",
    "    validation_examples=validation_examples,\n",
    "    validation_targets=validation_targets)\n",
    "print(\"Model size:\", model_size(linear_classifier))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n",
    "                                                  validation_targets[\"victims_alert\"], \n",
    "                                                  num_epochs=1, \n",
    "                                                  shuffle=False)\n",
    "\n",
    "validation_predictions = linear_classifier.predict(input_fn=predict_validation_input_fn)\n",
    "validation_predictions = np.array([item['probabilities'][0] for item in validation_predictions])\n",
    "\n",
    "_ = plt.hist(validation_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrics = linear_classifier.evaluate(input_fn=predict_validation_input_fn)\n",
    "\n",
    "print(\"AUC on the validation set: {:0.4f}\".format(evaluation_metrics['auc']))\n",
    "print(\"Accuracy on the validation set: {:0.5f}\".format(evaluation_metrics['accuracy']))\n",
    "print(\"AUC_precision-recall {:0.3f}\".format(evaluation_metrics['auc_precision_recall']))\n",
    "\n",
    "validation_probabilities = linear_classifier.predict(input_fn=predict_validation_input_fn)\n",
    "# Get just the probabilities for the positive class.\n",
    "validation_probabilities = np.array([item['probabilities'][1] for item in validation_probabilities])\n",
    "\n",
    "false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(\n",
    "    validation_targets, validation_probabilities)\n",
    "plt.plot(false_positive_rate, true_positive_rate, label=\"our model\")\n",
    "plt.plot([0, 1], [0, 1], label=\"random classifier\")\n",
    "_ = plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision = average_precision_score(validation_targets, validation_probabilities)\n",
    "precision, recall, _ = precision_recall_curve(validation_targets, validation_probabilities)\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1 =LogisticRegression()\n",
    "lr.fit(X_test, y_test)\n",
    "prediction = lr.predict_proba(X_resampled)\n",
    "accuracy_score(y_train, prediction)\n",
    "recall_score(y_resampled, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Logistic Regression\", \"Random Forest\", \"Gradient Boost Classifier\"]\n",
    "classifiers = [\n",
    "    LogisticRegression(C= 2),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    GradientBoostingClassifier()]\n",
    "#model_list = ['logreg']\n",
    "logreg_score = []\n",
    "rf_score = []\n",
    "gbc_score = []\n",
    "logreg_recall = []\n",
    "rf_recall = []\n",
    "gbc_recall = []\n",
    "accidents.drop(['#_deaths', 'amount_victims', 'incident_#', 'minor_injures', 'severely_injured', 'victims'],\\\n",
    "               axis=1, inplace=True)\n",
    "\n",
    "\n",
    "objectes = accidents.select_dtypes(include='object').columns\n",
    "numerical = accidents.select_dtypes(exclude='object').columns\n",
    "print('Object_columns:', objectes, len(objectes))\n",
    "print('Numerical_columns:', numerical, len(numerical))\n",
    "print('Ha de ser zero:', len(accidents.columns) - len(numerical) - len(objectes))\n",
    "dummies = pd.get_dummies(accidents[objectes])\n",
    "final = pd.concat([accidents[numerical], dummies], axis=1)\n",
    "\n",
    "correlation = final.corr()\n",
    "#correlation['victims_alert'].head(10)\n",
    "\n",
    "pos_values = []\n",
    "for item in correlation['victims_alert']:\n",
    "    if item < 0:\n",
    "        item = item * (-1)\n",
    "        pos_values.append(item)\n",
    "    else:\n",
    "        pos_values.append(item)\n",
    "correlation['victims_alert'] = pos_values\n",
    "all_features = list(correlation['victims_alert'].sort_values(ascending=False)[1::].index)\n",
    "for num in range(2,len(all_features), 2):\n",
    "    \n",
    "    fea = all_features[0:num]\n",
    "    X = final[fea]\n",
    "    y = accidents['victims_alert'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    if (y.sum()/final.shape[0]) < 0.05:\n",
    "    \n",
    "        X_resampled, y_resampled = SMOTE(ratio=0.3, random_state=2018).fit_sample(X_train, y_train)\n",
    "        scaler = StandardScaler()\n",
    "        Xs_train = scaler.fit_transform(X_resampled)\n",
    "        Xs_test = scaler.transform(X_test)\n",
    "\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "        Xs_train = scaler.fit_transform(X_train)\n",
    "        Xs_test = scaler.transform(X_test)\n",
    "\n",
    "    \n",
    "    for name, clf in zip(names, classifiers):\n",
    "        \n",
    "        clf.fit(Xs_train, y_resampled)\n",
    "        score = clf.score(Xs_test, y_test)\n",
    "        pred = clf.predict(Xs_test)\n",
    "        prob = clf.predict_proba(X_test)\n",
    "        new_proba = []\n",
    "        for l in range(0,len(prob)):\n",
    "            z = prob[l][1]\n",
    "            new_proba.append(z)\n",
    "        \n",
    "        \n",
    "\n",
    "            hedging_all = y.sum() * (2)\n",
    "            new_pred = [hedging_all + 1]\n",
    "\n",
    "            limits = np.linspace(0.0253,0.03, 20)\n",
    "            i = 0\n",
    "            \n",
    "            while i < len(limits) and sum(new_pred) > hedging_all:\n",
    "\n",
    "                numero = limits[i]\n",
    "    \n",
    "                #for numero in np.linspace(0.0253,0.03, 20):\n",
    "                #new_pred = [1960]\n",
    "                    #while sum(new_pred) > y.sum():\n",
    "                new_pred = []\n",
    "                for item in new_proba:\n",
    "                    if item > numero:\n",
    "                        new_pred.append(1)\n",
    "                    else:\n",
    "                        new_pred.append(0)\n",
    "\n",
    "                i += 1\n",
    "\n",
    "    \n",
    "            recall = recall_score(y_test, new_pred)\n",
    "\n",
    "    \n",
    "        #recall = recall_score(y_test, pred)\n",
    "        print(name, score)\n",
    "        if name == 'Logistic Regression':\n",
    "            logreg_score.append(score)\n",
    "            logreg_recall.append(recall)\n",
    "            print(name, score)\n",
    "        elif name == \"Random Forest\":\n",
    "            rf_score.append(score)\n",
    "            rf_recall.append(recall)\n",
    "        elif name == \"Gradient Boost Classifier\":\n",
    "            gbc_score.append(score)\n",
    "            gbc_recall.append(recall)\n",
    "            \n",
    "\"\"\"X = accidents[['amount_vehicles_involved', 'day_month',\n",
    "        'hour_day', 'point_x', 'point_y',\n",
    "        'street_code', 'year']]\n",
    "y = accidents['victims_alert'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=2018)\"\"\"\n",
    "\n",
    "#pred = lr.predict(X_test)\n",
    "\n",
    "\n",
    "####METRICS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = []\n",
    "for item in range(2, len(all_features), 2):\n",
    "    x.append(item)\n",
    "plt.plot(x, logreg_score, c='y')\n",
    "plt.plot(x, rf_score, c='r')\n",
    "plt.plot(x, gbc_score, c='b')\n",
    "plt.axhline(baseline);\n",
    "\n",
    "###Change metrics: no accuracy but to attend all the requested cases TruePositives/all_positives is \n",
    "###called tru positive rate or recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxim_logreg = 0\n",
    "for features,y in zip(x, logreg_score):\n",
    "    if y > maxim_logreg:\n",
    "        maxim_logreg = y\n",
    "print(maxim_logreg)\n",
    "\n",
    "logreg_serie = pd.Series(data=logreg_score, index=x)\n",
    "print('logreg max:', logreg_serie[logreg_serie == maxim_logreg])\n",
    "\n",
    "maxim_rf = 0\n",
    "for features,y in zip(x, rf_score):\n",
    "    if y > maxim_rf:\n",
    "        maxim_rf = y\n",
    "#print(maxim_rf)\n",
    "\n",
    "rf_serie = pd.Series(data=rf_score, index=x)\n",
    "print('rf max:', rf_serie[rf_serie == maxim_rf])\n",
    "\n",
    "maxim_gbc = 0\n",
    "for features,y in zip(x, gbc_score):\n",
    "    if y > maxim_gbc:\n",
    "        maxim_gbc = y\n",
    "#print(maxim_rf)\n",
    "\n",
    "gbc_serie = pd.Series(data=gbc_score, index=x)\n",
    "print('rf max:', gbc_serie[gbc_serie == maxim_rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model == str(logreg):\n",
    "            \n",
    "            \n",
    "            lr = LogisticRegression()\n",
    "            \n",
    "            lr.fit(Xs_train, y_train)\n",
    "            pred = lr.predict(Xs_test)\n",
    "            score = accuracy_score(y_test, pred)\n",
    "            logreg_score.append(score)\n",
    "        \n",
    "        elif model == randomforest:\n",
    "            \n",
    "            rf = RandomForestClassifier()\n",
    "            rf.fit(Xs_train, y_train)\n",
    "            pred = rf.predict(Xs_test)\n",
    "            score = accuracy_score(y_test, pred)\n",
    "            rf_score.append(score)\n",
    "        else:\n",
    "            \n",
    "            gbc = GradientBoostingClassifier()\n",
    "            gbc.fit(Xs_train, y_train)\n",
    "            pred = gbc.predict(Xs_test)\n",
    "            score = accuracy_score(y_test, pred)\n",
    "            gbc_score.append(score)\n",
    "            \n",
    "plt.plot(range(range(2,len(all_features), 2)), logreg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation['victims_alert'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_feature_columns():\n",
    "    \"\"\"Construct the TensorFlow Feature Columns.\n",
    "\n",
    "    Returns:\n",
    "    A set of feature columns\n",
    "    \"\"\"\n",
    "\n",
    "    bucketized_households = tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column(\"households\"),\n",
    "    boundaries=get_quantile_based_buckets(training_examples[\"households\"], 10))\n",
    "    bucketized_longitude = tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column(\"longitude\"),\n",
    "    boundaries=get_quantile_based_buckets(training_examples[\"longitude\"], 50))\n",
    "    bucketized_latitude = tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column(\"latitude\"),\n",
    "    boundaries=get_quantile_based_buckets(training_examples[\"latitude\"], 50))\n",
    "    bucketized_housing_median_age = tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column(\"housing_median_age\"),\n",
    "    boundaries=get_quantile_based_buckets(\n",
    "      training_examples[\"housing_median_age\"], 10))\n",
    "    bucketized_total_rooms = tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column(\"total_rooms\"),\n",
    "    boundaries=get_quantile_based_buckets(training_examples[\"total_rooms\"], 10))\n",
    "    bucketized_total_bedrooms = tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column(\"total_bedrooms\"),\n",
    "    boundaries=get_quantile_based_buckets(training_examples[\"total_bedrooms\"], 10))\n",
    "    bucketized_population = tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column(\"population\"),\n",
    "    boundaries=get_quantile_based_buckets(training_examples[\"population\"], 10))\n",
    "    bucketized_median_income = tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column(\"median_income\"),\n",
    "    boundaries=get_quantile_based_buckets(training_examples[\"median_income\"], 10))\n",
    "    bucketized_rooms_per_person = tf.feature_column.bucketized_column(\n",
    "    tf.feature_column.numeric_column(\"rooms_per_person\"),\n",
    "    boundaries=get_quantile_based_buckets(\n",
    "      training_examples[\"rooms_per_person\"], 10))\n",
    "\n",
    "    long_x_lat = tf.feature_column.crossed_column(\n",
    "    set([bucketized_longitude, bucketized_latitude]), hash_bucket_size=1000)\n",
    "\n",
    "    feature_columns = set([\n",
    "    long_x_lat,\n",
    "    bucketized_longitude,\n",
    "    bucketized_latitude,\n",
    "    bucketized_housing_median_age,\n",
    "    bucketized_total_rooms,\n",
    "    bucketized_total_bedrooms,\n",
    "    bucketized_population,\n",
    "    bucketized_households,\n",
    "    bucketized_median_income,\n",
    "    bucketized_rooms_per_person])\n",
    "  \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_class(target, features, data, model):\n",
    "    \n",
    "    \n",
    "    \n",
    "    objectes = data[features].select_dtypes(include='object').columns\n",
    "    numericals = data[features].select_dtypes(exclude='object').columns\n",
    "    #dummies = pd.get_dummies(data[objectes])\n",
    "    #final = pd.concat([data[numericals], dummies], axis=1)\n",
    "    #list(features).remove(target)\n",
    "    X = data[prova_features]\n",
    "    y = data[target].reshape(-1,1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=2018)\n",
    "    if (y.sum()/len(data)) < 0.05:\n",
    "        \n",
    "        X_resampled, y_resampled = SMOTE(ratio=0.3, random_state=218).fit_sample(X_train, y_train)\n",
    "        scaler = StandardScaler()\n",
    "        Xs_train = scaler.fit_transform(X_resampled)\n",
    "        Xs_test = scaler.transform(X_test)\n",
    "    params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': np.linspace(0.01, 0.99, 5)}\n",
    "\n",
    "    grid_search = GridSearchCV(model,\n",
    "                          params,\n",
    "                          verbose=1,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "    grid_search.fit(Xs_train, y_resampled)\n",
    "    print(grid_search.best_params_)\n",
    "    lr_hyperparams = grid_search.best_estimator_\n",
    "    pred = grid_search.predict(Xs_test)\n",
    "    score = accuracy_score(y_test, pred)\n",
    "    conf = classification_report(y_test, pred)\n",
    "    baseline = 1 - (y.sum()/len(data))\n",
    "    false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(\n",
    "    validation_targets, validation_probabilities)\n",
    "    plt.plot(false_positive_rate, true_positive_rate, label=\"our model\")\n",
    "    plt.plot([0, 1], [0, 1], label=\"random classifier\")\n",
    "    _ = plt.legend(loc=2)\n",
    "    \n",
    "    return 'Baseline:', baseline, 'Score:', score, lr_hyperparams\n",
    "\n",
    "\n",
    "binary_class('victims_alert', prova_features, df2, LogisticRegression(random_state=2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = list(first_set_features)\n",
    "xx.append('visca')\n",
    "tt = 'visca'\n",
    "print(xx)\n",
    "xx.remove(tt, target)\n",
    "print(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(first_set_features))\n",
    "tt = 'victims_alert'\n",
    "prova_features = [x for x in first_set_features if x != tt]\n",
    "prova_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###LogReg\n",
    "target = accidents['victims_alert']\n",
    "X = final[all_features[0:12]]\n",
    "y = target.reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=2018)\n",
    "\n",
    "if (y.sum()/len(accidents)) < 0.05:\n",
    "        \n",
    "    X_resampled, y_resampled = SMOTE(ratio=0.3, random_state=2018).fit_sample(X_train, y_train)\n",
    "    scaler = StandardScaler()\n",
    "    Xs_train = scaler.fit_transform(X_resampled)\n",
    "    Xs_test = scaler.transform(X_test)\n",
    "params = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': np.linspace(0.01, 0.99, 5)}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(random_state=2018),\n",
    "                    params,\n",
    "                    verbose=2,\n",
    "                    n_jobs=-1)\n",
    "\n",
    "grid_search.fit(Xs_train, y_resampled)\n",
    "print(grid_search.best_params_)\n",
    "lr_hyperparams = grid_search.best_estimator_\n",
    "pred_logreg = grid_search.predict(Xs_test)\n",
    "score_logreg = accuracy_score(y_test, pred)\n",
    "conf = classification_report(y_test, pred)\n",
    "baseline = 1 - (y.sum()/len(accidents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_logreg.sum(), score_logreg, baseline-score_logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###RAndom Forest\n",
    "\n",
    "target = accidents['victims_alert']\n",
    "objectes = accidents[initial_features].select_dtypes(include='object').columns\n",
    "numericals = accidents[initial_features].select_dtypes(exclude='object').columns\n",
    "dummies = pd.get_dummies(accidents[objectes])\n",
    "final = pd.concat([accidents[numericals], dummies], axis=1)\n",
    "X = final\n",
    "y = target.reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=2018)\n",
    "\n",
    "if (y.sum()/len(accidents)) < 0.05:\n",
    "        \n",
    "    X_resampled, y_resampled = SMOTE(ratio=0.3, random_state=2018).fit_sample(X_train, y_train)\n",
    "    scaler = StandardScaler()\n",
    "    Xs_train = scaler.fit_transform(X_resampled)\n",
    "    Xs_test = scaler.transform(X_test)\n",
    "params = {\n",
    "    'max_depth': [2,3,4, 5],\n",
    "    'n_estimators': [6, 8, 10, 12, 14]}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=2018,),\n",
    "                    params,\n",
    "                    verbose=2,\n",
    "                    n_jobs=-1)\n",
    "\n",
    "grid_search.fit(Xs_train, y_resampled)\n",
    "print(grid_search.best_params_)\n",
    "lr_hyperparams = grid_search.best_estimator_\n",
    "pred_rf = grid_search.predict(Xs_test)\n",
    "score_rf = accuracy_score(y_test, pred)\n",
    "conf = classification_report(y_test, pred)\n",
    "baseline = 1 - (y.sum()/len(accidents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline, score_rf, pred_rf.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Confusion matrix being 1 predicted sevre\n",
    "# True Negative. There are no injured and we predict no injures\n",
    "conf_matrix['pred'] = pred_rf\n",
    "conf_matrix['true'] = y_test\n",
    "\n",
    "tn = 0 ### predicted no injures and there are no injures\n",
    "tp = 0 ### predicted injures and there are injures\n",
    "fn = 0 ### predicted no injures but there are injures\n",
    "fp = 0 ### predicted injures there are none\n",
    "\n",
    "for index, row in conf_matrix[['pred', 'true']].iterrows():\n",
    "    \n",
    "    if row['pred'] == 0 and (row['true'] == 0):\n",
    "        \n",
    "        tn += 1\n",
    "    \n",
    "    elif row['pred'] == 1 and row['true'] == 1:\n",
    "        \n",
    "        tp += 1\n",
    "        \n",
    "    elif row['pred'] == 0 and row['true'] == 1:\n",
    "        \n",
    "        fn +=1\n",
    "        \n",
    "    elif row['pred'] == 1 and row['true'] == 0:\n",
    "        \n",
    "        fp +=1\n",
    "    \n",
    "print(tn, tp, fn, fp)\n",
    "print('Has to be zero:', len(pred) - (tn+tp+fn+fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Gradient Boost Classifier\n",
    "target = accidents['victims_alert']\n",
    "objectes = accidents[initial_features].select_dtypes(include='object').columns\n",
    "numericals = accidents[initial_features].select_dtypes(exclude='object').columns\n",
    "dummies = pd.get_dummies(accidents[objectes])\n",
    "final = pd.concat([accidents[numericals], dummies], axis=1)\n",
    "X = final\n",
    "y = target.reshape(-1,1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=2018)\n",
    "\n",
    "if (y.sum()/len(accidents)) < 0.05:\n",
    "        \n",
    "    X_resampled, y_resampled = SMOTE(ratio=0.3, random_state=2018).fit_sample(X_train, y_train)\n",
    "    scaler = StandardScaler()\n",
    "    Xs_train = scaler.fit_transform(X_resampled)\n",
    "    Xs_test = scaler.transform(X_test)\n",
    "params = {\n",
    "    'loss': ['deviance','exponential'],\n",
    "    'max_depth': [2,3,4, 5]}\n",
    "\n",
    "grid_search = GridSearchCV(GradientBoostingClassifier(random_state=2018,),\n",
    "                    params,\n",
    "                    verbose=2,\n",
    "                    n_jobs=-1)\n",
    "\n",
    "grid_search.fit(Xs_train, y_resampled)\n",
    "print(grid_search.best_params_)\n",
    "lr_hyperparams = grid_search.best_estimator_\n",
    "pred_gbc = grid_search.predict(Xs_test)\n",
    "score_gbc = accuracy_score(y_test, pred)\n",
    "conf = classification_report(y_test, pred)\n",
    "baseline = 1 - (y.sum()/len(accidents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llista = [1960 * 1.1]\n",
    "llista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = accidents[['amount_vehicles_involved', 'day_month',\n",
    "        'hour_day', 'point_x', 'point_y',\n",
    "        'street_code', 'year']]\n",
    "y = accidents['victims_alert'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=2018)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "pred = lr.predict(X_test)\n",
    "prob = lr.predict_proba(X_test)\n",
    "prob_1 = []\n",
    "\n",
    "####METRICS\n",
    "\n",
    "\n",
    "new_proba = []\n",
    "for l in range(0,len(prob)):\n",
    "    z = prob[l][1]\n",
    "    new_proba.append(z)\n",
    "len(new_proba), max(new_proba), min(new_proba)\n",
    "\n",
    "hedging_all = y.sum() * (2)\n",
    "new_pred = [hedging_all + 1]\n",
    "\n",
    "limits = np.linspace(0.0253,0.03, 20)\n",
    "i = 0\n",
    "\n",
    "\n",
    "while i < len(limits) and sum(new_pred) > hedging_all:\n",
    "\n",
    "    numero = limits[i]\n",
    "    \n",
    "    #for numero in np.linspace(0.0253,0.03, 20):\n",
    "    #new_pred = [1960]\n",
    "        #while sum(new_pred) > y.sum():\n",
    "    new_pred = []\n",
    "    for item in new_proba:\n",
    "        if item > numero:\n",
    "            new_pred.append(1)\n",
    "        else:\n",
    "            new_pred.append(0)\n",
    "\n",
    "    i += 1\n",
    "    \n",
    "    print(sum(new_class),numero)\n",
    "recall_score(y_test, new_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_class = []\n",
    "\n",
    "for item in new_proba:\n",
    "    \n",
    "        #while sum(new_class) < y.sum():\n",
    "    if item > 0.027353:\n",
    "        new_class.append(1)\n",
    "    else:\n",
    "        new_class.append(0)\n",
    "            #print(num)\n",
    "    \n",
    "print(sum(new_class), y.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, new_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_class = [1960]\n",
    "\n",
    "limits = np.linspace(0.0253,0.03, 20)\n",
    "\n",
    "\"\"\"\n",
    "for(i=0; i < len(limits); i++) {\n",
    "    if (sum(new_class) > y.sum()) {\n",
    "        break;\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for n in range(10):\n",
    "    print(n)\n",
    "\n",
    "... same as ...\n",
    "\n",
    "nums = list(range(10))\n",
    "i = 0\n",
    "while i < len(nums):\n",
    "    n = nums[i]\n",
    "    \n",
    "    i += 1\n",
    "\"\"\"\n",
    "\n",
    "#while iters < 100 && tolerance > 0.001:   \n",
    "\n",
    "# answer = 'n'\n",
    "# while answer != 'y':\n",
    "#    answer = raw_input('Do you want to quit?')\n",
    "\n",
    "# do {\n",
    "#     answer = raw_input('Do you want to quit?')\n",
    "# } while(answer != 'y')\n",
    "\n",
    "\n",
    "i = 0\n",
    "while i < len(limits) and sum(new_class) > y.sum():\n",
    "\n",
    "    numero = limits[i]\n",
    "    \n",
    "    #for numero in np.linspace(0.0253,0.03, 20):\n",
    "    #new_class = [1960]\n",
    "        #while sum(new_class) > y.sum():\n",
    "    new_class = []\n",
    "    for item in new_proba:\n",
    "        if item > numero:\n",
    "            new_class.append(1)\n",
    "        else:\n",
    "            new_class.append(0)\n",
    "\n",
    "    i += 1\n",
    "    \n",
    "    print(sum(new_class),numero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_proba.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while sum(new) > 3: \n",
    "    new=[]\n",
    "    for numero in np.linspace(0,5, 6):\n",
    "        \n",
    "        for item in llista:\n",
    "            if item > numero:\n",
    "                new.append(1)\n",
    "            else:\n",
    "                new.append(0)\n",
    "        print(numero, item, sum(new))\n",
    "    \n",
    "numero, item, sum(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " llista = [1,2,3, 4, 5, 6]\n",
    "y.sum(), sum(llista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row1 = [tn, fp]\n",
    "row2 = [fn, tp]\n",
    "neg = [tn, fn]\n",
    "pos = [fp,tp]\n",
    "neg, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = accidents['victims_alert']\n",
    "objectes = accidents[initial_features].select_dtypes(include='object').columns\n",
    "numericals = accidents[initial_features].select_dtypes(exclude='object').columns\n",
    "dummies = pd.get_dummies(accidents[objectes])\n",
    "final = pd.concat([accidents[numericals], dummies], axis=1)\n",
    "X = final\n",
    "y = target.values.reshape(-1,1)\n",
    "baseline = 1 - (y.sum()/len(accidents))\n",
    "\n",
    "####Start with models\n",
    "\n",
    "if model == logreg:\n",
    "     ####Figuring out the best train_size\n",
    "    \n",
    "    resultat = []\n",
    "    imb_ = np.linspace(0.3, 0.8, 6.0)\n",
    "    for item in imb_:\n",
    "        X_train, X_test, y_train, y_test =\\\n",
    "        train_test_split(X,y,random_state=2018, train_size=item)\n",
    "        scaler = StandardScaler()\n",
    "        Xs_train = scaler.fit_transform(X_train)\n",
    "        Xs_test = scaler.transform(X_test)\n",
    "        lr = LogisticRegressionCV()\n",
    "        lr.fit(Xs_train, y_train)\n",
    "        pred = lr.predict(Xs_test)\n",
    "        score = accuracy_score(y_test, pred)\n",
    "        resultat.append(score)\n",
    "    maxim_score = 0\n",
    "    maxim_ratio =0\n",
    "        for x,y in zip(imb_, resultat):\n",
    "            if y > maxim_score:\n",
    "                maxim_score = y\n",
    "                maxim_ratio = x\n",
    "                \n",
    "    if (y.sum()/len(accidents)) < 0.05:\n",
    "\n",
    "            resultat = []\n",
    "            imb_ = np.linspace(0.3, 0.8, 6.0)\n",
    "            for item in imb_:\n",
    "                X_train, X_test, y_train, y_test = train_test_split\\\n",
    "                (X,y,random_state=2018, train_size=maxim_ratio)\n",
    "                X_resampled, y_resampled = SMOTE(ratio=item, random_state=218).fit_sample(X_train, y_train)\n",
    "                scaler = StandardScaler()\n",
    "                Xs_train = scaler.fit_transform(X_resampled)\n",
    "                Xs_test = scaler.transform(X_test)\n",
    "                lr = LogisticRegressionCV()\n",
    "                lr.fit(Xs_train, y_resampled)\n",
    "                pred = lr.predict(Xs_test)\n",
    "                score = accuracy_score(y_test, pred)\n",
    "                resultat.append(score)\n",
    "                #print(score)\n",
    "            plt.axhline(baseline, c='r')\n",
    "            plt.plot(np.linspace(0.3,0.8,6.0), resultat)\n",
    "            maxim_score = 0\n",
    "            train_ratio =0\n",
    "            for x,y in zip(imb_, resultat):\n",
    "                if y > maxim_score:\n",
    "                    maxim_score = y\n",
    "                    train_ratio = x\n",
    "\n",
    "            #print(maxim_score,maxim_ratio)\n",
    "\n",
    "    ### LogisticRegression\n",
    "            X_resampled, y_resampled = SMOTE(ratio=maxim_ratio, random_state=218).fit_sample(X_train, y_train)\n",
    "            scaler = StandardScaler()\n",
    "            Xs_train = scaler.fit_transform(X_resampled)\n",
    "            Xs_test = scaler.transform(X_test)\n",
    "            params = {\n",
    "                'penalty': ['l1', 'l2'],\n",
    "                'C': np.linspace(0.01, 0.99, 5)}\n",
    "\n",
    "            grid_search = GridSearchCV(LogisticRegression(),\n",
    "                                  params,\n",
    "                                  verbose=1,\n",
    "                                  n_jobs=-1)\n",
    "\n",
    "            grid_search.fit(Xs_train, y_resampled)\n",
    "            #print(grid_search.best_params_)\n",
    "            lr_hyperparams = grid_search.best_estimator_\n",
    "            pred = grid_search.predict(Xs_test)\n",
    "            scores = accuracy_score(y_test, pred)\n",
    "            conf = classification_report(y_test, pred)\n",
    "            print('LogisticRegression. Smote', grid_search.best_params_, scores) \n",
    "    else:\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        Xs_train = scaler.fit_transform(X_train)\n",
    "        Xs_test = scaler.transform(X_test)\n",
    "        params = {\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'C': np.linspace(0.01, 0.99, 5)}\n",
    "\n",
    "        grid_search = GridSearchCV(LogisticRegression(),\n",
    "                              params,\n",
    "                              verbose=1,\n",
    "                              n_jobs=-1)\n",
    "\n",
    "        grid_search.fit(Xs_train, y_train)\n",
    "        print(grid_search.best_params_)\n",
    "        lr_hyperparams = grid_search.best_estimator_\n",
    "        pred = grid_search.predict(Xs_test)\n",
    "        scores = accuracy_score(y_test, pred)\n",
    "        conf = classification_report(y_test, pred)\n",
    "        print('LogisticRegression. No smote', grid_search.best_params_, scores)\n",
    "        \n",
    "#elif model == KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
